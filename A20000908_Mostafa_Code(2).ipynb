{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Mostafa Walid Mostafa Radwan - A20000908**"
      ],
      "metadata": {
        "id": "Udeaaf2rvqWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the User-Item Matrix from a CSV file\n",
        "user_item_matrix = pd.read_csv('user_item_ratings.csv')\n",
        "\n",
        "# Strip any whitespace from the column names\n",
        "user_item_matrix.columns = user_item_matrix.columns.str.strip()\n",
        "\n",
        "# Get the number of users (tnu) and items (tni)\n",
        "tnu = user_item_matrix.shape[1] - 1  # Exclude the first column (user IDs)\n",
        "tni = user_item_matrix.shape[0]\n",
        "\n",
        "print(\"Number of Users (tnu):\", tnu)\n",
        "print(\"Number of Items (tni):\", tni)\n",
        "\n",
        "# Define the active users\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Initialize result storage\n",
        "results = []\n",
        "\n",
        "for active_user in active_users:\n",
        "    if active_user not in user_item_matrix.columns:\n",
        "        print(f\"Warning: {active_user} not found in the matrix columns.\")\n",
        "        continue\n",
        "\n",
        "    # Get the items rated by the active user\n",
        "    active_user_rated = user_item_matrix[active_user].notna()\n",
        "    co_rated_users = 0\n",
        "    co_rated_items = 0\n",
        "\n",
        "    for other_user in user_item_matrix.columns:\n",
        "        if active_user == other_user:\n",
        "            continue\n",
        "\n",
        "        # Get the items rated by the other user\n",
        "        other_user_rated = user_item_matrix[other_user].notna()\n",
        "\n",
        "        # Find common rated items\n",
        "        common_rated_items = active_user_rated & other_user_rated\n",
        "        common_count = common_rated_items.sum()\n",
        "\n",
        "        if common_count > 0:\n",
        "            co_rated_users += 1\n",
        "            co_rated_items += common_count\n",
        "\n",
        "    results.append({\n",
        "        'Active_User': active_user,\n",
        "        'No_common_users': co_rated_users,\n",
        "        'No_coRated_items': co_rated_items\n",
        "    })\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nResults:\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JE54nKHyQnZ",
        "outputId": "73fbd7fc-18e5-4a3e-c353-c86b6f4eeabb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Users (tnu): 50\n",
            "Number of Items (tni): 30\n",
            "\n",
            "Results:\n",
            "  Active_User  No_common_users  No_coRated_items\n",
            "0       User1               50              1128\n",
            "1       User2               50              1205\n",
            "2       User3               50              1259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the User-Item Matrix from a CSV file\n",
        "user_item_matrix = pd.read_csv('user_item_ratings.csv')\n",
        "\n",
        "# Define the active users\n",
        "active_users = ['User1', 'User2', 'User3']\n",
        "\n",
        "# Strip any whitespace from the column names\n",
        "user_item_matrix.columns = user_item_matrix.columns.str.strip()\n",
        "active_users = [user.strip() for user in active_users]\n",
        "\n",
        "# Initialize result storage\n",
        "results = []\n",
        "\n",
        "for active_user in active_users:\n",
        "    if active_user not in user_item_matrix.columns:\n",
        "        print(f\"Warning: {active_user} not found in the matrix columns.\")\n",
        "        continue\n",
        "\n",
        "    # Get the items rated by the active user\n",
        "    active_user_rated = user_item_matrix[active_user].notna()\n",
        "    co_rated_users = 0\n",
        "    co_rated_items = 0\n",
        "\n",
        "    for other_user in user_item_matrix.columns:\n",
        "        if active_user == other_user:\n",
        "            continue\n",
        "\n",
        "        # Get the items rated by the other user\n",
        "        other_user_rated = user_item_matrix[other_user].notna()\n",
        "\n",
        "        # Find common rated items\n",
        "        common_rated_items = active_user_rated & other_user_rated\n",
        "        common_count = common_rated_items.sum()\n",
        "\n",
        "        if common_count > 0:\n",
        "            co_rated_users += 1\n",
        "            co_rated_items += common_count\n",
        "\n",
        "    results.append({\n",
        "        'Active_User': active_user,\n",
        "        'No_common_users': co_rated_users,\n",
        "        'No_coRated_items': co_rated_items\n",
        "    })\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Sort by \"No_common_users\" in descending order\n",
        "sorted_results = results_df.sort_values(by='No_common_users', ascending=False)\n",
        "\n",
        "# Create a 2-D array\n",
        "result_array = sorted_results[['No_common_users', 'No_coRated_items']].to_numpy()\n",
        "\n",
        "# Display the 2-D array\n",
        "print(\"2-D Array:\")\n",
        "print(result_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_OKLdX6gXl1",
        "outputId": "9f0784f8-ce52-4e33-c48a-0ef3be1c03f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2-D Array:\n",
            "[[  50 1128]\n",
            " [  50 1205]\n",
            " [  50 1259]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the User-Item Matrix from a CSV file\n",
        "user_item_matrix = pd.read_csv('user_item_ratings.csv')\n",
        "\n",
        "# Count the number of ratings for each item (row-wise count of non-NA values)\n",
        "item_rating_counts = user_item_matrix.notna().sum(axis=1)\n",
        "\n",
        "# Sort the counts in descending order\n",
        "sorted_counts = item_rating_counts.sort_values(ascending=False)\n",
        "\n",
        "# Plot the curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(sorted_counts.values, marker='o', linestyle='-', color='b')\n",
        "plt.title('Quantity of Ratings for Each Item', fontsize=16)\n",
        "plt.xlabel('Item Index (Sorted by Quantity of Ratings)', fontsize=12)\n",
        "plt.ylabel('Number of Ratings', fontsize=12)\n",
        "plt.grid(alpha=0.4)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "JZWCj8p9gaqj",
        "outputId": "22ea9c0f-963c-4e4e-f8fe-617c222c22a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the User-Item Matrix from a CSV file\n",
        "user_item_matrix = pd.read_csv('user_item_ratings.csv', index_col=0)\n",
        "\n",
        "# Define active users (replace with actual user identifiers in your dataset)\n",
        "active_users = ['User1', 'User2', 'User3']\n",
        "\n",
        "# Get the total number of items\n",
        "total_items = user_item_matrix.shape[0]\n",
        "\n",
        "# Dictionary to store thresholds for each active user\n",
        "thresholds = {}\n",
        "\n",
        "for active_user in active_users:\n",
        "    # Get the items rated by the active user\n",
        "    active_user_rated = user_item_matrix[active_user].notna()\n",
        "\n",
        "    # Count the number of items rated by the active user\n",
        "    num_items_rated_by_active_user = active_user_rated.sum()\n",
        "\n",
        "    # Set the 30% co-rating threshold\n",
        "    min_co_rated_items = int(0.3 * num_items_rated_by_active_user)\n",
        "\n",
        "    # Initialize counter for users who co-rated at least 30% of the items\n",
        "    max_common_users = 0\n",
        "\n",
        "    # Iterate through all other users to check co-rating\n",
        "    for user in user_item_matrix.columns:\n",
        "        if user == active_user:\n",
        "            continue\n",
        "\n",
        "        # Get items rated by the current user\n",
        "        user_rated = user_item_matrix[user].notna()\n",
        "\n",
        "        # Find the number of common items rated by both users\n",
        "        co_rated_items = (active_user_rated & user_rated).sum()\n",
        "\n",
        "        # Check if this user meets the 30% threshold\n",
        "        if co_rated_items >= min_co_rated_items:\n",
        "            max_common_users += 1\n",
        "\n",
        "    # Store the result for the active user\n",
        "    thresholds[active_user] = max_common_users\n",
        "\n",
        "# Display the thresholds\n",
        "for user, threshold in thresholds.items():\n",
        "    print(f\"Threshold for {user}: {threshold}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0ZbQL70hBPP",
        "outputId": "3e5b55b8-f696-4b1b-d2e6-00cec7267232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold for User1: 49\n",
            "Threshold for User2: 49\n",
            "Threshold for User3: 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def count_common_users_and_co_rated_items(user_ratings, active_user, threshold=0.3):\n",
        "\n",
        "    if active_user not in user_ratings.columns:\n",
        "        return f\"Warning: {active_user} not found in the matrix columns.\"\n",
        "\n",
        "    # Get the items rated by the active user\n",
        "    active_user_rated = user_ratings[active_user].notna()\n",
        "\n",
        "    no_common_users = 0\n",
        "    no_co_rated_items = 0\n",
        "\n",
        "    for other_user in user_ratings.columns:\n",
        "        if active_user != other_user:\n",
        "            other_user_rated = user_ratings[other_user].notna()\n",
        "            common_rated_items = active_user_rated & other_user_rated\n",
        "            common_count = common_rated_items.sum()\n",
        "\n",
        "            total_ratings_active_user = active_user_rated.sum()\n",
        "            percentage_co_rated = common_count / total_ratings_active_user\n",
        "\n",
        "            if percentage_co_rated >= threshold:\n",
        "                no_common_users += 1\n",
        "                no_co_rated_items += common_count\n",
        "\n",
        "    return {\n",
        "        'Active_User': active_user,\n",
        "        'No_common_users': no_common_users,\n",
        "        'No_coRated_items': no_co_rated_items\n",
        "    }\n",
        "\n",
        "# Assuming your data is in a CSV file with columns as users\n",
        "user_ratings = pd.read_csv('user_item_ratings.csv', index_col=0)\n",
        "\n",
        "active_users = ['User1', 'User2', 'User3']\n",
        "\n",
        "for active_user in active_users:\n",
        "    result = count_common_users_and_co_rated_items(user_ratings, active_user, threshold=0.3)\n",
        "    if isinstance(result, str):  # Check if result is a warning message\n",
        "        print(result)\n",
        "    else:\n",
        "        print(f\"\\nFor Active User: {result['Active_User']}\")\n",
        "        print(f\"No_common_users (with at least 30% co-rated items): {result['No_common_users']}\")\n",
        "        print(f\"No_coRated_items (with at least 30% co-rated users): {result['No_coRated_items']}\")"
      ],
      "metadata": {
        "id": "6lgodn5dh4xK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec617e8-845a-42ee-81a8-705c82f8c9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "For Active User: User1\n",
            "No_common_users (with at least 30% co-rated items): 49\n",
            "No_coRated_items (with at least 30% co-rated users): 1103\n",
            "\n",
            "For Active User: User2\n",
            "No_common_users (with at least 30% co-rated items): 49\n",
            "No_coRated_items (with at least 30% co-rated users): 1178\n",
            "\n",
            "For Active User: User3\n",
            "No_common_users (with at least 30% co-rated items): 49\n",
            "No_coRated_items (with at least 30% co-rated users): 1231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 1**\n",
        "**Case Study 1.1**"
      ],
      "metadata": {
        "id": "HpAMh4kLJ5W3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "User-based Collaborative Filtering algorithms using Cosine Similarity without considering the bias adjustment effect of mean-centering"
      ],
      "metadata": {
        "id": "9hfq1h0BiNoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Fill NaN with 0 for cosine similarity calculations\n",
        "user_item_filled = user_item_matrix.fillna(0)\n",
        "\n",
        "# Compute cosine similarity between users\n",
        "similarity_matrix = pd.DataFrame(\n",
        "    cosine_similarity(user_item_filled),\n",
        "    index=user_item_filled.index,\n",
        "    columns=user_item_filled.index\n",
        ")\n",
        "\n",
        "# Function to find top N% closest users\n",
        "def get_top_n_percent(similarity_row, n=20):\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)\n",
        "    return sorted_users.iloc[:top_n_cutoff]\n",
        "\n",
        "# Function to predict ratings\n",
        "def predict_rating(active_user, similarity_matrix, user_item_matrix, use_discount_factor=False):\n",
        "    top_users = get_top_n_percent(similarity_matrix.loc[active_user], n=20)\n",
        "\n",
        "    if use_discount_factor:\n",
        "        # Apply discount factor\n",
        "        max_similarity = top_users.max()\n",
        "        discount_factor = 1 - (top_users / max_similarity)\n",
        "        discounted_similarity = top_users * (1 - discount_factor)\n",
        "    else:\n",
        "        discounted_similarity = top_users\n",
        "\n",
        "    # Predict ratings\n",
        "    predicted_ratings = {}\n",
        "    for item in user_item_matrix.columns:\n",
        "        if pd.notna(user_item_matrix.loc[active_user, item]):\n",
        "            continue  # Skip items already rated by the active user\n",
        "\n",
        "        numerator = (user_item_matrix[item] * discounted_similarity).sum()\n",
        "        denominator = discounted_similarity.abs().sum()\n",
        "\n",
        "        predicted_ratings[item] = numerator / denominator if denominator != 0 else 0\n",
        "\n",
        "    return predicted_ratings\n",
        "\n",
        "# Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nAnalysis for {active_user}:\\n\")\n",
        "\n",
        "    # Step 1.1.3: Prediction without Discount Factor\n",
        "    predictions_no_df = predict_rating(\n",
        "        active_user, similarity_matrix, user_item_matrix, use_discount_factor=False\n",
        "    )\n",
        "    print(\"Predictions without Discount Factor:\", predictions_no_df)\n",
        "\n",
        "    # Step 1.1.6: Prediction with Discount Factor\n",
        "    predictions_with_df = predict_rating(\n",
        "        active_user, similarity_matrix, user_item_matrix, use_discount_factor=True\n",
        "    )\n",
        "    print(\"Predictions with Discount Factor:\", predictions_with_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_EQkeV0NNQq",
        "outputId": "e61430e9-fb5c-4180-8337-5023bae3fb34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analysis for User1:\n",
            "\n",
            "Predictions without Discount Factor: {'Come the Morning': 2.534482131719024, 'The Perfect Shadow': 2.5114620827470824, \"Maybe It's True What They Say About Us\": 1.8509512700093147, 'The Problem with People': 2.3552674525712014, 'Amityville: Where the Echo Lives': 2.0550285545440676}\n",
            "Predictions with Discount Factor: {'Come the Morning': 2.4555325840648674, 'The Perfect Shadow': 2.4111508036425175, \"Maybe It's True What They Say About Us\": 1.7923522193038968, 'The Problem with People': 2.2969731319363573, 'Amityville: Where the Echo Lives': 1.9988659764297974}\n",
            "\n",
            "Analysis for User2:\n",
            "\n",
            "Predictions without Discount Factor: {'I giganti del cielo': 2.047197222509709, \"L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\": 2.166088000898524, 'Power Alley': 2.8397333074613496}\n",
            "Predictions with Discount Factor: {'I giganti del cielo': 1.9878300703155134, \"L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\": 2.1246575704310477, 'Power Alley': 2.7697924185550367}\n",
            "\n",
            "Analysis for User3:\n",
            "\n",
            "Predictions without Discount Factor: {'Nine Ball': 1.5641825722488452, 'Moe': 2.539599322277086}\n",
            "Predictions with Discount Factor: {'Nine Ball': 1.5237666273777077, 'Moe': 2.4720070415627187}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the top 20% closest users to each active user."
      ],
      "metadata": {
        "id": "AEjF5F0m_ByV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Fill NaN with 0 for cosine similarity calculations\n",
        "user_item_filled = user_item_matrix.fillna(0)\n",
        "\n",
        "# Compute cosine similarity between users\n",
        "similarity_matrix = pd.DataFrame(\n",
        "    cosine_similarity(user_item_filled),\n",
        "    index=user_item_filled.index,\n",
        "    columns=user_item_filled.index\n",
        ")\n",
        "\n",
        "# Function to find top N% closest users\n",
        "def get_top_n_percent(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on cosine similarity.\n",
        "    \"\"\"\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)  # Sort users by similarity (highest first)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nTop 20% Closest Users for {active_user}:\\n\")\n",
        "\n",
        "    # Get the similarity row for the active user\n",
        "    similarity_row = similarity_matrix.loc[active_user]\n",
        "\n",
        "    # Get the top 20% closest users\n",
        "    top_20_percent_users = get_top_n_percent(similarity_row, n=20)\n",
        "\n",
        "    print(top_20_percent_users)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM3clZSM4D20",
        "outputId": "9a6789d5-e15e-4a80-e195-83ba77347eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20% Closest Users for User1:\n",
            "\n",
            "User1     1.000000\n",
            "User16    0.811344\n",
            "User5     0.805661\n",
            "User25    0.796926\n",
            "User4     0.793058\n",
            "User34    0.790270\n",
            "User49    0.789581\n",
            "User38    0.773793\n",
            "User46    0.764264\n",
            "User17    0.764253\n",
            "Name: User1, dtype: float64\n",
            "\n",
            "Top 20% Closest Users for User2:\n",
            "\n",
            "User2     1.000000\n",
            "User28    0.886142\n",
            "User38    0.840459\n",
            "User45    0.830957\n",
            "User25    0.821160\n",
            "User19    0.817527\n",
            "User39    0.815440\n",
            "User21    0.799950\n",
            "User12    0.799177\n",
            "User23    0.791842\n",
            "Name: User2, dtype: float64\n",
            "\n",
            "Top 20% Closest Users for User3:\n",
            "\n",
            "User3     1.000000\n",
            "User29    0.861920\n",
            "User17    0.844031\n",
            "User23    0.830241\n",
            "User22    0.829816\n",
            "User35    0.828374\n",
            "User12    0.828334\n",
            "User14    0.828183\n",
            "User25    0.826934\n",
            "User6     0.825333\n",
            "Name: User3, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the prediction for each active user to find whether the user will like or dislike the not yet seen or rated items."
      ],
      "metadata": {
        "id": "eOvqT_ey_jvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Fill NaN with 0 for cosine similarity calculations\n",
        "user_item_filled = user_item_matrix.fillna(0)\n",
        "\n",
        "# Compute cosine similarity between users\n",
        "similarity_matrix = pd.DataFrame(\n",
        "    cosine_similarity(user_item_filled),\n",
        "    index=user_item_filled.index,\n",
        "    columns=user_item_filled.index\n",
        ")\n",
        "\n",
        "# Function to find top N% closest users\n",
        "def get_top_n_percent(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on cosine similarity.\n",
        "    \"\"\"\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)  # Sort users by similarity (highest first)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Function to predict ratings for unrated items\n",
        "def predict_rating(active_user, similarity_matrix, user_item_matrix, use_discount_factor=False):\n",
        "    top_users = get_top_n_percent(similarity_matrix.loc[active_user], n=20)\n",
        "\n",
        "    # Predict ratings for unrated items\n",
        "    predicted_ratings = {}\n",
        "    for item in user_item_matrix.columns:\n",
        "        if pd.notna(user_item_matrix.loc[active_user, item]):\n",
        "            continue  # Skip items already rated by the active user\n",
        "\n",
        "        # Get ratings of top N% closest users for this item\n",
        "        top_user_ratings = user_item_matrix[item].loc[top_users.index]\n",
        "\n",
        "        # Calculate weighted average of ratings, using similarity as weights\n",
        "        numerator = (top_user_ratings * top_users).sum()\n",
        "        denominator = top_users.sum()\n",
        "\n",
        "        # Predict the rating for the active user and item\n",
        "        predicted_rating = numerator / denominator if denominator != 0 else 0\n",
        "        predicted_ratings[item] = predicted_rating\n",
        "\n",
        "    return predicted_ratings\n",
        "\n",
        "# Function to classify as \"like\" or \"dislike\"\n",
        "def classify_likes_dislikes(predicted_ratings, threshold=3):\n",
        "    \"\"\"\n",
        "    Classify the predicted ratings as 'like' or 'dislike' based on a threshold.\n",
        "    \"\"\"\n",
        "    classified_predictions = {}\n",
        "    for item, rating in predicted_ratings.items():\n",
        "        classification = 'like' if rating > threshold else 'dislike'\n",
        "        classified_predictions[item] = classification\n",
        "    return classified_predictions\n",
        "\n",
        "# Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nAnalysis for {active_user}:\\n\")\n",
        "\n",
        "    # Step 1.1.3: Prediction without Discount Factor\n",
        "    predictions_no_df = predict_rating(\n",
        "        active_user, similarity_matrix, user_item_matrix, use_discount_factor=False\n",
        "    )\n",
        "    print(\"Predictions without Discount Factor:\", predictions_no_df)\n",
        "\n",
        "    # Step 1.1.6: Classify predictions as 'like' or 'dislike'\n",
        "    classified_predictions_no_df = classify_likes_dislikes(predictions_no_df)\n",
        "    print(\"Classified Predictions without Discount Factor:\", classified_predictions_no_df)\n",
        "\n",
        "    # Step 1.1.6: Prediction with Discount Factor (if you choose to apply discounting)\n",
        "    predictions_with_df = predict_rating(\n",
        "        active_user, similarity_matrix, user_item_matrix, use_discount_factor=True\n",
        "    )\n",
        "    print(\"Predictions with Discount Factor:\", predictions_with_df)\n",
        "\n",
        "    # Step 1.1.6: Classify predictions with 'like' or 'dislike'\n",
        "    classified_predictions_with_df = classify_likes_dislikes(predictions_with_df)\n",
        "    print(\"Classified Predictions with Discount Factor:\", classified_predictions_with_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa9VbESn4cGL",
        "outputId": "2d3482a1-e68a-4c85-a624-fdd35dc87741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analysis for User1:\n",
            "\n",
            "Predictions without Discount Factor: {'Come the Morning': 2.5344821317190243, 'The Perfect Shadow': 2.5114620827470824, \"Maybe It's True What They Say About Us\": 1.8509512700093147, 'The Problem with People': 2.3552674525712014, 'Amityville: Where the Echo Lives': 2.0550285545440676}\n",
            "Classified Predictions without Discount Factor: {'Come the Morning': 'dislike', 'The Perfect Shadow': 'dislike', \"Maybe It's True What They Say About Us\": 'dislike', 'The Problem with People': 'dislike', 'Amityville: Where the Echo Lives': 'dislike'}\n",
            "Predictions with Discount Factor: {'Come the Morning': 2.5344821317190243, 'The Perfect Shadow': 2.5114620827470824, \"Maybe It's True What They Say About Us\": 1.8509512700093147, 'The Problem with People': 2.3552674525712014, 'Amityville: Where the Echo Lives': 2.0550285545440676}\n",
            "Classified Predictions with Discount Factor: {'Come the Morning': 'dislike', 'The Perfect Shadow': 'dislike', \"Maybe It's True What They Say About Us\": 'dislike', 'The Problem with People': 'dislike', 'Amityville: Where the Echo Lives': 'dislike'}\n",
            "\n",
            "Analysis for User2:\n",
            "\n",
            "Predictions without Discount Factor: {'I giganti del cielo': 2.047197222509709, \"L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\": 2.1660880008985237, 'Power Alley': 2.8397333074613496}\n",
            "Classified Predictions without Discount Factor: {'I giganti del cielo': 'dislike', \"L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\": 'dislike', 'Power Alley': 'dislike'}\n",
            "Predictions with Discount Factor: {'I giganti del cielo': 2.047197222509709, \"L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\": 2.1660880008985237, 'Power Alley': 2.8397333074613496}\n",
            "Classified Predictions with Discount Factor: {'I giganti del cielo': 'dislike', \"L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\": 'dislike', 'Power Alley': 'dislike'}\n",
            "\n",
            "Analysis for User3:\n",
            "\n",
            "Predictions without Discount Factor: {'Nine Ball': 1.5641825722488456, 'Moe': 2.5395993222770854}\n",
            "Classified Predictions without Discount Factor: {'Nine Ball': 'dislike', 'Moe': 'dislike'}\n",
            "Predictions with Discount Factor: {'Nine Ball': 1.5641825722488456, 'Moe': 2.5395993222770854}\n",
            "Classified Predictions with Discount Factor: {'Nine Ball': 'dislike', 'Moe': 'dislike'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the discount factor (DF) then the discounted similarity (DS), for each of the active users considering the threshold in each case."
      ],
      "metadata": {
        "id": "vf_MH0ebAhIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Fill NaN with 0 for cosine similarity calculations\n",
        "user_item_filled = user_item_matrix.fillna(0)\n",
        "\n",
        "# Compute cosine similarity between users\n",
        "similarity_matrix = pd.DataFrame(\n",
        "    cosine_similarity(user_item_filled),\n",
        "    index=user_item_filled.index,\n",
        "    columns=user_item_filled.index\n",
        ")\n",
        "\n",
        "# Function to find top N% closest users\n",
        "def get_top_n_percent(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on cosine similarity.\n",
        "    \"\"\"\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)  # Sort users by similarity (highest first)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Function to compute discount factor and discounted similarity\n",
        "def compute_discounted_similarity(active_user, similarity_matrix, n=20, threshold=3):\n",
        "    \"\"\"\n",
        "    Compute discount factor (DF) and discounted similarity (DS) for each active user.\n",
        "    \"\"\"\n",
        "    # Get top N% closest users\n",
        "    top_users = get_top_n_percent(similarity_matrix.loc[active_user], n=n)\n",
        "\n",
        "    # Compute the discount factor and discounted similarity for each user\n",
        "    max_similarity = top_users.max()  # Max similarity score among top N% closest users\n",
        "    discount_factors = {}\n",
        "    discounted_similarities = {}\n",
        "\n",
        "    for user, similarity in top_users.items():\n",
        "        # Calculate discount factor (DF)\n",
        "        DF = 1 - (similarity / max_similarity)\n",
        "\n",
        "        # Calculate discounted similarity (DS)\n",
        "        DS = similarity * (1 - DF)\n",
        "\n",
        "        # Store the results\n",
        "        discount_factors[user] = DF\n",
        "        discounted_similarities[user] = DS\n",
        "\n",
        "    return discount_factors, discounted_similarities\n",
        "\n",
        "# Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nAnalysis for {active_user}:\\n\")\n",
        "\n",
        "    # Compute discount factor and discounted similarity for each user\n",
        "    discount_factors, discounted_similarities = compute_discounted_similarity(\n",
        "        active_user, similarity_matrix, n=20, threshold=3\n",
        "    )\n",
        "\n",
        "    # Output the results\n",
        "    print(\"Discount Factor (DF):\", discount_factors)\n",
        "    print(\"Discounted Similarity (DS):\", discounted_similarities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXkeU56d4xHT",
        "outputId": "2b16552a-894c-4c13-e1ad-57f3b8b221c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analysis for User1:\n",
            "\n",
            "Discount Factor (DF): {'User1': 0.0, 'User16': 0.18865605447444544, 'User5': 0.1943390510551144, 'User25': 0.20307420307380464, 'User4': 0.20694201873034024, 'User34': 0.2097298678803371, 'User49': 0.21041907915637204, 'User38': 0.22620742320963805, 'User46': 0.23573632183086213, 'User17': 0.23574709942726157}\n",
            "Discounted Similarity (DS): {'User1': 1.0000000000000002, 'User16': 0.6582789979409742, 'User5': 0.6490895646547737, 'User25': 0.6350907258064518, 'User4': 0.6289409616555082, 'User34': 0.6245268817204296, 'User49': 0.6234380305602717, 'User38': 0.5987549518958684, 'User46': 0.5840989697686197, 'User17': 0.5840824960338442}\n",
            "\n",
            "Analysis for User2:\n",
            "\n",
            "Discount Factor (DF): {'User2': 0.0, 'User28': 0.11385774483235056, 'User38': 0.15954108301729952, 'User45': 0.16904345736894155, 'User25': 0.1788402070408247, 'User19': 0.1824731568734851, 'User39': 0.18455954597335855, 'User21': 0.20005040759052406, 'User12': 0.20082290035617067, 'User23': 0.20815775373009726}\n",
            "Discounted Similarity (DS): {'User2': 1.0, 'User28': 0.7852480963936075, 'User38': 0.7063711911357338, 'User45': 0.690488775741362, 'User25': 0.6743034055727556, 'User19': 0.6683501392324053, 'User39': 0.6649431340631752, 'User21': 0.6399193503960867, 'User12': 0.6386840365951231, 'User23': 0.6270141429777653}\n",
            "\n",
            "Analysis for User3:\n",
            "\n",
            "Discount Factor (DF): {'User3': 0.0, 'User29': 0.13808048240192294, 'User17': 0.15596851794285882, 'User23': 0.16975859243616287, 'User22': 0.17018384700273836, 'User35': 0.17162574847620793, 'User12': 0.17166569719842917, 'User14': 0.1718167018921476, 'User25': 0.17306589521219884, 'User6': 0.17466691353863983}\n",
            "Discounted Similarity (DS): {'User3': 1.0000000000000002, 'User29': 0.742905254816502, 'User17': 0.7123891427035745, 'User23': 0.6893007948335818, 'User22': 0.6885948477751749, 'User35': 0.6862039005876029, 'User12': 0.6861377171977646, 'User14': 0.6858875752648002, 'User25': 0.6838200136612023, 'User6': 0.6811747036078352}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the top 20% closest users using Discounted Similarity"
      ],
      "metadata": {
        "id": "WrF_EVdYBbx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Fill NaN with 0 for cosine similarity calculations\n",
        "user_item_filled = user_item_matrix.fillna(0)\n",
        "\n",
        "# Compute cosine similarity between users\n",
        "similarity_matrix = pd.DataFrame(\n",
        "    cosine_similarity(user_item_filled),\n",
        "    index=user_item_filled.index,\n",
        "    columns=user_item_filled.index\n",
        ")\n",
        "\n",
        "# Function to find top N% closest users based on discounted similarity\n",
        "def get_top_n_percent_by_ds(similarity_matrix, active_user, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity matrix and an active user, return the top N% closest users based on discounted similarity.\n",
        "    \"\"\"\n",
        "    # Compute discount factor and discounted similarity\n",
        "    top_users = get_top_n_percent(similarity_matrix.loc[active_user], n=100)  # Get the top 100 users first\n",
        "\n",
        "    max_similarity = top_users.max()  # Max similarity score among top N% closest users\n",
        "    discount_factors = {}\n",
        "    discounted_similarities = {}\n",
        "\n",
        "    for user, similarity in top_users.items():\n",
        "        # Calculate discount factor (DF)\n",
        "        DF = 1 - (similarity / max_similarity)\n",
        "\n",
        "        # Calculate discounted similarity (DS)\n",
        "        DS = similarity * (1 - DF)\n",
        "\n",
        "        # Store the results\n",
        "        discounted_similarities[user] = DS\n",
        "\n",
        "    # Sort the users based on discounted similarity\n",
        "    sorted_users_by_ds = pd.Series(discounted_similarities).sort_values(ascending=False)\n",
        "\n",
        "    # Select the top N% closest users\n",
        "    top_n_cutoff = int(len(sorted_users_by_ds) * n / 100)\n",
        "    return sorted_users_by_ds.iloc[:top_n_cutoff]  # Return top N% closest users based on discounted similarity\n",
        "\n",
        "# Function to compute discount factor and discounted similarity\n",
        "def compute_discounted_similarity(active_user, similarity_matrix, n=20):\n",
        "    \"\"\"\n",
        "    Compute discount factor (DF) and discounted similarity (DS) for each active user.\n",
        "    \"\"\"\n",
        "    # Get the top N% closest users based on discounted similarity\n",
        "    top_users_by_ds = get_top_n_percent_by_ds(similarity_matrix, active_user, n=n)\n",
        "    return top_users_by_ds\n",
        "\n",
        "# Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nAnalysis for {active_user}:\\n\")\n",
        "\n",
        "    # Compute the top 20% closest users based on discounted similarity\n",
        "    top_20_percent_users = compute_discounted_similarity(\n",
        "        active_user, similarity_matrix, n=20\n",
        "    )\n",
        "\n",
        "    # Output the results\n",
        "    print(f\"Top 20% closest users to {active_user} based on discounted similarity:\")\n",
        "    print(top_20_percent_users)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2pW8IoT5ndD",
        "outputId": "0cb96d85-6dc4-4e9f-c127-d389f9f9f9ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analysis for User1:\n",
            "\n",
            "Top 20% closest users to User1 based on discounted similarity:\n",
            "User1     1.000000\n",
            "User16    0.658279\n",
            "User5     0.649090\n",
            "User25    0.635091\n",
            "User4     0.628941\n",
            "User34    0.624527\n",
            "User49    0.623438\n",
            "User38    0.598755\n",
            "User46    0.584099\n",
            "User17    0.584082\n",
            "dtype: float64\n",
            "\n",
            "Analysis for User2:\n",
            "\n",
            "Top 20% closest users to User2 based on discounted similarity:\n",
            "User2     1.000000\n",
            "User28    0.785248\n",
            "User38    0.706371\n",
            "User45    0.690489\n",
            "User25    0.674303\n",
            "User19    0.668350\n",
            "User39    0.664943\n",
            "User21    0.639919\n",
            "User12    0.638684\n",
            "User23    0.627014\n",
            "dtype: float64\n",
            "\n",
            "Analysis for User3:\n",
            "\n",
            "Top 20% closest users to User3 based on discounted similarity:\n",
            "User3     1.000000\n",
            "User29    0.742905\n",
            "User17    0.712389\n",
            "User23    0.689301\n",
            "User22    0.688595\n",
            "User35    0.686204\n",
            "User12    0.686138\n",
            "User14    0.685888\n",
            "User25    0.683820\n",
            "User6     0.681175\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the prediction for each active user to find whether the user will like or dislike the not yet seen or rated items.\n",
        "\n"
      ],
      "metadata": {
        "id": "fJV7Hx5fBq0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Fill NaN with 0 for cosine similarity calculations\n",
        "user_item_filled = user_item_matrix.fillna(0)\n",
        "\n",
        "# Compute cosine similarity between users\n",
        "similarity_matrix = pd.DataFrame(\n",
        "    cosine_similarity(user_item_filled),\n",
        "    index=user_item_filled.index,\n",
        "    columns=user_item_filled.index\n",
        ")\n",
        "\n",
        "# Function to find top N% closest users based on discounted similarity\n",
        "def get_top_n_percent_by_ds(similarity_matrix, active_user, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity matrix and an active user, return the top N% closest users based on discounted similarity.\n",
        "    \"\"\"\n",
        "    # Compute discount factor and discounted similarity\n",
        "    top_users = get_top_n_percent(similarity_matrix.loc[active_user], n=100)  # Get the top 100 users first\n",
        "\n",
        "    max_similarity = top_users.max()  # Max similarity score among top N% closest users\n",
        "    discount_factors = {}\n",
        "    discounted_similarities = {}\n",
        "\n",
        "    for user, similarity in top_users.items():\n",
        "        # Calculate discount factor (DF)\n",
        "        DF = 1 - (similarity / max_similarity)\n",
        "\n",
        "        # Calculate discounted similarity (DS)\n",
        "        DS = similarity * (1 - DF)\n",
        "\n",
        "        # Store the results\n",
        "        discounted_similarities[user] = DS\n",
        "\n",
        "    # Sort the users based on discounted similarity\n",
        "    sorted_users_by_ds = pd.Series(discounted_similarities).sort_values(ascending=False)\n",
        "\n",
        "    # Select the top N% closest users\n",
        "    top_n_cutoff = int(len(sorted_users_by_ds) * n / 100)\n",
        "    return sorted_users_by_ds.iloc[:top_n_cutoff]  # Return top N% closest users based on discounted similarity\n",
        "\n",
        "# Function to predict ratings for unrated items\n",
        "def predict_rating_for_unrated_items(active_user, similarity_matrix, user_item_matrix, n=20, threshold=3):\n",
        "    \"\"\"\n",
        "    Predict ratings for unrated items for each active user based on the top N% similar users.\n",
        "    If the predicted rating is above a certain threshold, the user will like the item.\n",
        "    \"\"\"\n",
        "    # Get the top N% closest users based on discounted similarity\n",
        "    top_users_by_ds = get_top_n_percent_by_ds(similarity_matrix, active_user, n=n)\n",
        "\n",
        "    predicted_ratings = {}\n",
        "\n",
        "    # Iterate over all items to predict the rating for unrated items\n",
        "    for item in user_item_matrix.columns:\n",
        "        if pd.notna(user_item_matrix.loc[active_user, item]):  # Skip if the item is already rated\n",
        "            continue\n",
        "\n",
        "        # Compute predicted rating using weighted average of top N% closest users' ratings\n",
        "        numerator = 0\n",
        "        denominator = 0\n",
        "        for user in top_users_by_ds.index:\n",
        "            rating = user_item_matrix.loc[user, item]\n",
        "            if pd.notna(rating):  # Only consider rated items\n",
        "                numerator += rating * top_users_by_ds[user]  # Weighted sum\n",
        "                denominator += abs(top_users_by_ds[user])  # Sum of weights\n",
        "\n",
        "        # Compute the predicted rating\n",
        "        if denominator != 0:\n",
        "            predicted_rating = numerator / denominator\n",
        "        else:\n",
        "            predicted_rating = 0  # Default to 0 if no ratings are available from the top users\n",
        "\n",
        "        # Predict whether the user will like or dislike the item\n",
        "        predicted_ratings[item] = 'Like' if predicted_rating >= threshold else 'Dislike'\n",
        "\n",
        "    return predicted_ratings\n",
        "\n",
        "# Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nAnalysis for {active_user}:\\n\")\n",
        "\n",
        "    # Predict whether the user will like or dislike unrated items\n",
        "    predictions = predict_rating_for_unrated_items(\n",
        "        active_user, similarity_matrix, user_item_matrix, n=20, threshold=3\n",
        "    )\n",
        "\n",
        "    # Output the predictions\n",
        "    print(f\"Predictions for {active_user}:\")\n",
        "    print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jzXFGBD5P0o",
        "outputId": "2ac9bba8-3b97-47ca-ff56-1e2bb27353ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analysis for User1:\n",
            "\n",
            "Predictions for User1:\n",
            "{'Come the Morning': 'Dislike', 'The Perfect Shadow': 'Dislike', \"Maybe It's True What They Say About Us\": 'Dislike', 'The Problem with People': 'Dislike', 'Amityville: Where the Echo Lives': 'Like'}\n",
            "\n",
            "Analysis for User2:\n",
            "\n",
            "Predictions for User2:\n",
            "{'I giganti del cielo': 'Dislike', \"L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\": 'Dislike', 'Power Alley': 'Like'}\n",
            "\n",
            "Analysis for User3:\n",
            "\n",
            "Predictions for User3:\n",
            "{'Nine Ball': 'Dislike', 'Moe': 'Like'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case Study 1.2**"
      ],
      "metadata": {
        "id": "9X4M5b8Ei2BY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "User-based Collaborative Filtering algorithms using Cosine Similarity with mean-centering"
      ],
      "metadata": {
        "id": "a_8DXNhkikp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Mean-Centering: Subtract each user's mean rating from their ratings\n",
        "mean_ratings = user_item_matrix.mean(axis=1)\n",
        "mean_centered_matrix = user_item_matrix.sub(mean_ratings, axis=0)\n",
        "\n",
        "# Fill NaN with 0 for cosine similarity calculations\n",
        "mean_centered_matrix_filled = mean_centered_matrix.fillna(0)\n",
        "\n",
        "# Cosine similarity with mean-centering\n",
        "similarity_matrix_mean_centered = pd.DataFrame(\n",
        "    cosine_similarity(mean_centered_matrix_filled),\n",
        "    index=mean_centered_matrix.index,\n",
        "    columns=mean_centered_matrix.index\n",
        ")\n",
        "\n",
        "# Function to find top N% closest users\n",
        "def get_top_n_percent(similarity_row, n=20):\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)\n",
        "    return sorted_users.iloc[:top_n_cutoff]\n",
        "\n",
        "# Function to predict ratings with or without discount factor\n",
        "def predict_rating(active_user, similarity_matrix, user_item_matrix, mean_ratings, use_discount_factor=False):\n",
        "    top_users = get_top_n_percent(similarity_matrix.loc[active_user], n=20)\n",
        "\n",
        "    if use_discount_factor:\n",
        "        # Apply discount factor\n",
        "        max_similarity = top_users.max()\n",
        "        discount_factor = 1 - (top_users / max_similarity)\n",
        "        discounted_similarity = top_users * (1 - discount_factor)\n",
        "    else:\n",
        "        discounted_similarity = top_users\n",
        "\n",
        "    # Predict ratings\n",
        "    predicted_ratings = {}\n",
        "    for item in user_item_matrix.columns:\n",
        "        if pd.notna(user_item_matrix.loc[active_user, item]):\n",
        "            continue  # Skip items already rated by the active user\n",
        "\n",
        "        # Weighted average of ratings from top users\n",
        "        numerator = (\n",
        "            (user_item_matrix[item] - mean_ratings[top_users.index]).fillna(0) * discounted_similarity\n",
        "        ).sum()\n",
        "        denominator = discounted_similarity.abs().sum()\n",
        "\n",
        "        # Add back the mean rating of the active user for prediction\n",
        "        predicted_ratings[item] = mean_ratings[active_user] + (numerator / denominator if denominator != 0 else 0)\n",
        "\n",
        "    return predicted_ratings\n",
        "\n",
        "# Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nAnalysis for {active_user}:\\n\")\n",
        "\n",
        "    # Step 1.2.3: Prediction without Discount Factor\n",
        "    predictions_no_df = predict_rating(\n",
        "        active_user, similarity_matrix_mean_centered, user_item_matrix, mean_ratings, use_discount_factor=False\n",
        "    )\n",
        "    print(\"Predictions without Discount Factor:\", predictions_no_df)\n",
        "\n",
        "    # Step 1.2.6: Prediction with Discount Factor\n",
        "    predictions_with_df = predict_rating(\n",
        "        active_user, similarity_matrix_mean_centered, user_item_matrix, mean_ratings, use_discount_factor=True\n",
        "    )\n",
        "    print(\"Predictions with Discount Factor:\", predictions_with_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD2qDNVBOCO3",
        "outputId": "97b8a4e5-1848-4c7c-9f97-4bc49703d10d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analysis for User1:\n",
            "\n",
            "Predictions without Discount Factor: {'Come the Morning': 3.535023635033543, 'The Perfect Shadow': 2.619884825845544, \"Maybe It's True What They Say About Us\": 3.455236088430259, 'The Problem with People': 3.5044475538467585, 'Amityville: Where the Echo Lives': 3.7481262291235744}\n",
            "Predictions with Discount Factor: {'Come the Morning': 3.4361428549285575, 'The Perfect Shadow': 2.856448867534542, \"Maybe It's True What They Say About Us\": 3.2717991120447114, 'The Problem with People': 3.345622592309169, 'Amityville: Where the Echo Lives': 3.465625008216889}\n",
            "\n",
            "Analysis for User2:\n",
            "\n",
            "Predictions without Discount Factor: {'I giganti del cielo': 2.6135442179981028, \"L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\": 3.1831849226496955, 'Power Alley': 3.250048753889893}\n",
            "Predictions with Discount Factor: {'I giganti del cielo': 2.73049714000584, \"L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\": 3.2160589556638026, 'Power Alley': 3.1859344484818406}\n",
            "\n",
            "Analysis for User3:\n",
            "\n",
            "Predictions without Discount Factor: {'Nine Ball': 3.8811216610513393, 'Moe': 3.601559963160354}\n",
            "Predictions with Discount Factor: {'Nine Ball': 3.7818758879638152, 'Moe': 3.4518283456448056}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the top 20% closest users to each active user."
      ],
      "metadata": {
        "id": "taicBX3J-4pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Mean-Centering: Subtract each user's mean rating from their ratings\n",
        "mean_ratings = user_item_matrix.mean(axis=1)\n",
        "mean_centered_matrix = user_item_matrix.sub(mean_ratings, axis=0)\n",
        "\n",
        "# Fill NaN with 0 for cosine similarity calculations\n",
        "mean_centered_matrix_filled = mean_centered_matrix.fillna(0)\n",
        "\n",
        "# Compute cosine similarity between users based on mean-centered ratings\n",
        "similarity_matrix_mean_centered = pd.DataFrame(\n",
        "    cosine_similarity(mean_centered_matrix_filled),\n",
        "    index=mean_centered_matrix.index,\n",
        "    columns=mean_centered_matrix.index\n",
        ")\n",
        "\n",
        "# Function to find top N% closest users based on cosine similarity\n",
        "def get_top_n_percent(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on cosine similarity.\n",
        "    \"\"\"\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)  # Sort users by similarity (highest first)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nTop 20% Closest Users for {active_user} based on mean-centered ratings:\\n\")\n",
        "\n",
        "    # Get the similarity row for the active user\n",
        "    similarity_row = similarity_matrix_mean_centered.loc[active_user]\n",
        "\n",
        "    # Get the top 20% closest users\n",
        "    top_20_percent_users = get_top_n_percent(similarity_row, n=20)\n",
        "\n",
        "    print(top_20_percent_users)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tes3U5QC8ncn",
        "outputId": "5b1c2dcb-ed94-4f14-cac9-4f8cc85c0786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20% Closest Users for User1 based on mean-centered ratings:\n",
            "\n",
            "User1     1.000000\n",
            "User34    0.335968\n",
            "User16    0.279364\n",
            "User10    0.274481\n",
            "User9     0.242951\n",
            "User6     0.242320\n",
            "User5     0.155236\n",
            "User29    0.147421\n",
            "User23    0.140747\n",
            "User15    0.129336\n",
            "Name: User1, dtype: float64\n",
            "\n",
            "Top 20% Closest Users for User2 based on mean-centered ratings:\n",
            "\n",
            "User2     1.000000\n",
            "User21    0.456713\n",
            "User28    0.444823\n",
            "User38    0.432251\n",
            "User43    0.389256\n",
            "User45    0.304393\n",
            "User25    0.247108\n",
            "User5     0.241456\n",
            "User19    0.216982\n",
            "User39    0.191035\n",
            "Name: User2, dtype: float64\n",
            "\n",
            "Top 20% Closest Users for User3 based on mean-centered ratings:\n",
            "\n",
            "User3     1.000000\n",
            "User24    0.575759\n",
            "User12    0.275748\n",
            "User26    0.267658\n",
            "User47    0.254834\n",
            "User35    0.238089\n",
            "User17    0.222731\n",
            "User42    0.204389\n",
            "User22    0.194101\n",
            "User32    0.123002\n",
            "Name: User3, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the prediction for each active user to find whether the user will like or dislike the not yet seen or rated items."
      ],
      "metadata": {
        "id": "NdfwgJfV_V7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Mean-Centering: Subtract each user's mean rating from their ratings\n",
        "mean_ratings = user_item_matrix.mean(axis=1)\n",
        "mean_centered_matrix = user_item_matrix.sub(mean_ratings, axis=0)\n",
        "\n",
        "# Fill NaN with 0 for cosine similarity calculations\n",
        "mean_centered_matrix_filled = mean_centered_matrix.fillna(0)\n",
        "\n",
        "# Compute cosine similarity between users based on mean-centered ratings\n",
        "similarity_matrix_mean_centered = pd.DataFrame(\n",
        "    cosine_similarity(mean_centered_matrix_filled),\n",
        "    index=mean_centered_matrix.index,\n",
        "    columns=mean_centered_matrix.index\n",
        ")\n",
        "\n",
        "# Function to find top N% closest users based on cosine similarity\n",
        "def get_top_n_percent(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on cosine similarity.\n",
        "    \"\"\"\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)  # Sort users by similarity (highest first)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Function to predict ratings for items the active user hasn't rated yet\n",
        "def predict_rating(active_user, similarity_matrix, user_item_matrix, mean_ratings, n=20):\n",
        "    top_users = get_top_n_percent(similarity_matrix.loc[active_user], n=n)\n",
        "\n",
        "    # Predict ratings for items the active user hasn't rated\n",
        "    predicted_ratings = {}\n",
        "    for item in user_item_matrix.columns:\n",
        "        if pd.notna(user_item_matrix.loc[active_user, item]):\n",
        "            continue  # Skip items already rated by the active user\n",
        "\n",
        "        # Weighted average of ratings from top users (with cosine similarity as weights)\n",
        "        numerator = 0\n",
        "        denominator = 0\n",
        "        for user in top_users.index:\n",
        "            if pd.notna(user_item_matrix.loc[user, item]):  # Only consider users who have rated the item\n",
        "                numerator += user_item_matrix.loc[user, item] * similarity_matrix.loc[active_user, user]\n",
        "                denominator += abs(similarity_matrix.loc[active_user, user])\n",
        "\n",
        "        # Avoid division by zero\n",
        "        predicted_ratings[item] = numerator / denominator if denominator != 0 else mean_ratings[active_user]\n",
        "\n",
        "    return predicted_ratings\n",
        "\n",
        "# Function to determine like/dislike based on a threshold (e.g., 3)\n",
        "def predict_like_dislike(predicted_ratings, threshold=3):\n",
        "    predictions = {}\n",
        "    for item, predicted_rating in predicted_ratings.items():\n",
        "        if predicted_rating >= threshold:\n",
        "            predictions[item] = \"Like\"\n",
        "        else:\n",
        "            predictions[item] = \"Dislike\"\n",
        "    return predictions\n",
        "\n",
        "# Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nPrediction for {active_user}:\\n\")\n",
        "\n",
        "    # Predict ratings for items the active user hasn't rated yet\n",
        "    predictions = predict_rating(active_user, similarity_matrix_mean_centered, user_item_matrix, mean_ratings, n=20)\n",
        "    print(\"Predicted Ratings:\", predictions)\n",
        "\n",
        "    # Predict whether the active user will like or dislike the items\n",
        "    like_dislike_predictions = predict_like_dislike(predictions, threshold=3)\n",
        "    print(\"Like/Dislike Predictions:\", like_dislike_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGdLvKRn-rKG",
        "outputId": "50231f0c-6034-43c1-a478-73aeaeef9efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prediction for User1:\n",
            "\n",
            "Predicted Ratings: {'Come the Morning': 3.545859995614502, 'The Perfect Shadow': 2.1070904603048555, \"Maybe It's True What They Say About Us\": 3.418624673146063, 'The Problem with People': 3.548795770879103, 'Amityville: Where the Echo Lives': 3.8683679994393705}\n",
            "Like/Dislike Predictions: {'Come the Morning': 'Like', 'The Perfect Shadow': 'Dislike', \"Maybe It's True What They Say About Us\": 'Like', 'The Problem with People': 'Like', 'Amityville: Where the Echo Lives': 'Like'}\n",
            "\n",
            "Prediction for User2:\n",
            "\n",
            "Predicted Ratings: {'I giganti del cielo': 2.4422895832380136, \"L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\": 3.2211961212654514, 'Power Alley': 3.28057767922894}\n",
            "Like/Dislike Predictions: {'I giganti del cielo': 'Dislike', \"L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\": 'Like', 'Power Alley': 'Like'}\n",
            "\n",
            "Prediction for User3:\n",
            "\n",
            "Predicted Ratings: {'Nine Ball': 4.152478011735186, 'Moe': 3.6696269892575395}\n",
            "Like/Dislike Predictions: {'Nine Ball': 'Like', 'Moe': 'Like'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the discount factor (DF) then the discounted similarity (DS), for each of the active users considering the threshold in each case."
      ],
      "metadata": {
        "id": "7FOMGm7TAXIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Mean-Centering: Subtract each user's mean rating from their ratings\n",
        "mean_ratings = user_item_matrix.mean(axis=1)\n",
        "mean_centered_matrix = user_item_matrix.sub(mean_ratings, axis=0)\n",
        "\n",
        "# Fill NaN with 0 for cosine similarity calculations\n",
        "mean_centered_matrix_filled = mean_centered_matrix.fillna(0)\n",
        "\n",
        "# Compute cosine similarity between users based on mean-centered ratings\n",
        "similarity_matrix_mean_centered = pd.DataFrame(\n",
        "    cosine_similarity(mean_centered_matrix_filled),\n",
        "    index=mean_centered_matrix.index,\n",
        "    columns=mean_centered_matrix.index\n",
        ")\n",
        "\n",
        "# Function to find top N% closest users based on cosine similarity\n",
        "def get_top_n_percent(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on cosine similarity.\n",
        "    \"\"\"\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)  # Sort users by similarity (highest first)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Function to compute the Discount Factor (DF) and Discounted Similarity (DS)\n",
        "def compute_discounted_similarity(active_user, similarity_matrix, n=20, threshold=3):\n",
        "    top_users = get_top_n_percent(similarity_matrix.loc[active_user], n=n)\n",
        "\n",
        "    # Get the maximum similarity score among the top N% closest users\n",
        "    max_similarity = top_users.max()\n",
        "\n",
        "    # Compute the Discount Factor (DF) and Discounted Similarity (DS)\n",
        "    discounted_similarity = {}\n",
        "    for user, similarity in top_users.items():\n",
        "        # Calculate the Discount Factor (DF)\n",
        "        df = 1 - (similarity / max_similarity)\n",
        "\n",
        "        # Calculate the Discounted Similarity (DS)\n",
        "        ds = similarity * (1 - df)\n",
        "\n",
        "        discounted_similarity[user] = ds\n",
        "\n",
        "    return discounted_similarity\n",
        "\n",
        "# Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nDiscounted Similarity for {active_user}:\\n\")\n",
        "\n",
        "    # Compute Discounted Similarity (DS) for the active user\n",
        "    discounted_similarity = compute_discounted_similarity(active_user, similarity_matrix_mean_centered, n=20, threshold=3)\n",
        "\n",
        "    print(\"Discounted Similarity (DS):\", discounted_similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp-YUO0Z_UME",
        "outputId": "7aa85afd-18aa-457b-b1a0-a2884742fc94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Discounted Similarity for User1:\n",
            "\n",
            "Discounted Similarity (DS): {'User1': 0.9999999999999997, 'User34': 0.11287461026571734, 'User16': 0.07804399253364773, 'User10': 0.07533982719718237, 'User9': 0.05902539574725008, 'User6': 0.058719046185170896, 'User5': 0.02409836065573771, 'User29': 0.02173290737130894, 'User23': 0.019809849789682137, 'User15': 0.016727785551767106}\n",
            "\n",
            "Discounted Similarity for User2:\n",
            "\n",
            "Discounted Similarity (DS): {'User2': 1.0, 'User21': 0.2085870101749694, 'User28': 0.19786793757987178, 'User38': 0.18684098218411943, 'User43': 0.1515204566675154, 'User45': 0.09265495005245451, 'User25': 0.06106214692159449, 'User5': 0.05830078346953952, 'User19': 0.04708130812507178, 'User39': 0.036494437842477065}\n",
            "\n",
            "Discounted Similarity for User3:\n",
            "\n",
            "Discounted Similarity (DS): {'User3': 0.9999999999999999, 'User24': 0.33149787535410763, 'User12': 0.07603718334759595, 'User26': 0.0716410300333813, 'User47': 0.06494038598323304, 'User35': 0.05668620667841173, 'User17': 0.04960912056841554, 'User42': 0.04177491155467946, 'User22': 0.037675322694870096, 'User32': 0.01512945190233311}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the top 20% closest users using Discounted Similarity"
      ],
      "metadata": {
        "id": "kboCb_2QBNzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Mean-Centering: Subtract each user's mean rating from their ratings\n",
        "mean_ratings = user_item_matrix.mean(axis=1)\n",
        "mean_centered_matrix = user_item_matrix.sub(mean_ratings, axis=0)\n",
        "\n",
        "# Fill NaN with 0 for cosine similarity calculations\n",
        "mean_centered_matrix_filled = mean_centered_matrix.fillna(0)\n",
        "\n",
        "# Compute cosine similarity between users based on mean-centered ratings\n",
        "similarity_matrix_mean_centered = pd.DataFrame(\n",
        "    cosine_similarity(mean_centered_matrix_filled),\n",
        "    index=mean_centered_matrix.index,\n",
        "    columns=mean_centered_matrix.index\n",
        ")\n",
        "\n",
        "# Function to find top N% closest users based on cosine similarity\n",
        "def get_top_n_percent(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on cosine similarity.\n",
        "    \"\"\"\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)  # Sort users by similarity (highest first)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Function to compute the Discount Factor (DF) and Discounted Similarity (DS)\n",
        "def compute_discounted_similarity(active_user, similarity_matrix, n=20):\n",
        "    # Get the top N% closest users\n",
        "    top_users = get_top_n_percent(similarity_matrix.loc[active_user], n=n)\n",
        "\n",
        "    # Get the maximum similarity score among the top N% closest users\n",
        "    max_similarity = top_users.max()\n",
        "\n",
        "    # Compute the Discount Factor (DF) and Discounted Similarity (DS)\n",
        "    discounted_similarity = {}\n",
        "    for user, similarity in top_users.items():\n",
        "        # Calculate the Discount Factor (DF)\n",
        "        df = 1 - (similarity / max_similarity)\n",
        "\n",
        "        # Calculate the Discounted Similarity (DS)\n",
        "        ds = similarity * (1 - df)\n",
        "\n",
        "        discounted_similarity[user] = ds\n",
        "\n",
        "    return discounted_similarity\n",
        "\n",
        "# Function to determine the top 20% closest users based on discounted similarity\n",
        "def get_top_20_percent_discounted_similarity(active_user, similarity_matrix, n=20):\n",
        "    # Compute Discounted Similarity (DS) for the active user\n",
        "    discounted_similarity = compute_discounted_similarity(active_user, similarity_matrix, n)\n",
        "\n",
        "    # Sort the discounted similarities and get the top 20% closest users\n",
        "    sorted_discounted_similarity = pd.Series(discounted_similarity).sort_values(ascending=False)\n",
        "    top_20_percent_users = sorted_discounted_similarity.head(int(len(sorted_discounted_similarity) * 20 / 100))\n",
        "\n",
        "    return top_20_percent_users\n",
        "\n",
        "# Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nTop 20% Closest Users for {active_user} based on Discounted Similarity:\\n\")\n",
        "\n",
        "    # Get the top 20% closest users using the discounted similarity\n",
        "    top_20_percent_users = get_top_20_percent_discounted_similarity(active_user, similarity_matrix_mean_centered, n=100)\n",
        "\n",
        "    print(top_20_percent_users)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcENaFWhAWfA",
        "outputId": "abb36eb7-6fa5-47b6-977e-0076880e6367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20% Closest Users for User1 based on Discounted Similarity:\n",
            "\n",
            "User1     1.000000\n",
            "User34    0.112875\n",
            "User37    0.090606\n",
            "User41    0.085127\n",
            "User43    0.082998\n",
            "User21    0.079049\n",
            "User16    0.078044\n",
            "User10    0.075340\n",
            "User9     0.059025\n",
            "User6     0.058719\n",
            "dtype: float64\n",
            "\n",
            "Top 20% Closest Users for User2 based on Discounted Similarity:\n",
            "\n",
            "User2     1.000000\n",
            "User21    0.208587\n",
            "User28    0.197868\n",
            "User32    0.192795\n",
            "User38    0.186841\n",
            "User43    0.151520\n",
            "User45    0.092655\n",
            "User25    0.061062\n",
            "User5     0.058301\n",
            "User34    0.054787\n",
            "dtype: float64\n",
            "\n",
            "Top 20% Closest Users for User3 based on Discounted Similarity:\n",
            "\n",
            "User3     1.000000\n",
            "User24    0.331498\n",
            "User40    0.143221\n",
            "User34    0.083912\n",
            "User48    0.078121\n",
            "User12    0.076037\n",
            "User26    0.071641\n",
            "User18    0.070622\n",
            "User47    0.064940\n",
            "User35    0.056686\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the prediction for each active user to find whether the user will like or dislike the not yet seen or rated items."
      ],
      "metadata": {
        "id": "g2ejKk_gBfGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Mean-Centering: Subtract each user's mean rating from their ratings\n",
        "mean_ratings = user_item_matrix.mean(axis=1)\n",
        "mean_centered_matrix = user_item_matrix.sub(mean_ratings, axis=0)\n",
        "\n",
        "# Fill NaN with 0 for cosine similarity calculations\n",
        "mean_centered_matrix_filled = mean_centered_matrix.fillna(0)\n",
        "\n",
        "# Compute cosine similarity between users based on mean-centered ratings\n",
        "similarity_matrix_mean_centered = pd.DataFrame(\n",
        "    cosine_similarity(mean_centered_matrix_filled),\n",
        "    index=mean_centered_matrix.index,\n",
        "    columns=mean_centered_matrix.index\n",
        ")\n",
        "\n",
        "# Function to find top N% closest users based on cosine similarity\n",
        "def get_top_n_percent(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on cosine similarity.\n",
        "    \"\"\"\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)  # Sort users by similarity (highest first)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Function to compute the Discounted Similarity (DS)\n",
        "def compute_discounted_similarity(active_user, similarity_matrix, n=20):\n",
        "    # Get the top N% closest users\n",
        "    top_users = get_top_n_percent(similarity_matrix.loc[active_user], n=n)\n",
        "\n",
        "    # Get the maximum similarity score among the top N% closest users\n",
        "    max_similarity = top_users.max()\n",
        "\n",
        "    # Compute the Discounted Similarity (DS)\n",
        "    discounted_similarity = {}\n",
        "    for user, similarity in top_users.items():\n",
        "        # Calculate the Discount Factor (DF)\n",
        "        df = 1 - (similarity / max_similarity)\n",
        "\n",
        "        # Calculate the Discounted Similarity (DS)\n",
        "        ds = similarity * (1 - df)\n",
        "\n",
        "        discounted_similarity[user] = ds\n",
        "\n",
        "    return discounted_similarity\n",
        "\n",
        "# Function to predict ratings and determine if the user will like or dislike an item\n",
        "def predict_like_dislike(active_user, similarity_matrix, user_item_matrix, mean_ratings, threshold=3, n=20):\n",
        "    # Compute Discounted Similarity (DS) for the active user\n",
        "    discounted_similarity = compute_discounted_similarity(active_user, similarity_matrix, n)\n",
        "\n",
        "    # Predict ratings for items not yet rated by the active user\n",
        "    predictions = {}\n",
        "    for item in user_item_matrix.columns:\n",
        "        if pd.notna(user_item_matrix.loc[active_user, item]):\n",
        "            continue  # Skip items already rated by the active user\n",
        "\n",
        "        # Get the ratings of the top N% closest users\n",
        "        top_users = get_top_n_percent(similarity_matrix.loc[active_user], n=n)\n",
        "        numerator = sum((user_item_matrix[item].loc[user] - mean_ratings[user]) * discounted_similarity[user] for user in top_users.index if pd.notna(user_item_matrix.loc[user, item]))\n",
        "        denominator = sum(abs(discounted_similarity[user]) for user in top_users.index if pd.notna(user_item_matrix.loc[user, item]))\n",
        "\n",
        "        # Predict the rating as a weighted average\n",
        "        predicted_rating = mean_ratings[active_user] + (numerator / denominator if denominator != 0 else 0)\n",
        "\n",
        "        # Classify as like or dislike based on threshold\n",
        "        predictions[item] = \"Like\" if predicted_rating >= threshold else \"Dislike\"\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nPredictions for {active_user}:\\n\")\n",
        "\n",
        "    # Get the prediction for each not yet rated item (like/dislike)\n",
        "    predictions = predict_like_dislike(\n",
        "        active_user, similarity_matrix_mean_centered, user_item_matrix, mean_ratings, threshold=3, n=20\n",
        "    )\n",
        "\n",
        "    print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtHTOaGJBM0t",
        "outputId": "0fadd12b-8ef5-4d67-870c-07a9e8b82741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions for User1:\n",
            "\n",
            "{'Come the Morning': 'Like', 'The Perfect Shadow': 'Dislike', \"Maybe It's True What They Say About Us\": 'Like', 'The Problem with People': 'Like', 'Amityville: Where the Echo Lives': 'Like'}\n",
            "\n",
            "Predictions for User2:\n",
            "\n",
            "{'I giganti del cielo': 'Dislike', \"L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\": 'Like', 'Power Alley': 'Like'}\n",
            "\n",
            "Predictions for User3:\n",
            "\n",
            "{'Nine Ball': 'Like', 'Moe': 'Like'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case Study 1.3**"
      ],
      "metadata": {
        "id": "0eWEc1X9i6R5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "User-based Collaborative Filtering algorithms using Pearson Correlation Coefficient (PCC)"
      ],
      "metadata": {
        "id": "2JJSLCP6irHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Mean-Centering: Subtract each user's mean rating from their ratings\n",
        "mean_ratings = user_item_matrix.mean(axis=1)\n",
        "mean_centered_matrix = user_item_matrix.sub(mean_ratings, axis=0)\n",
        "\n",
        "# Pearson Correlation Coefficient (PCC) calculation\n",
        "pcc_matrix = mean_centered_matrix.T.corr(method='pearson')  # Transpose to correlate users, not items\n",
        "\n",
        "# Function to find top N% closest users based on PCC\n",
        "def get_top_n_percent_pcc(similarity_row, n=20):\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)\n",
        "    return sorted_users.iloc[:top_n_cutoff]\n",
        "\n",
        "# Function to predict ratings using PCC with or without discount factor\n",
        "def predict_rating_pcc(active_user, similarity_matrix, user_item_matrix, mean_ratings, use_discount_factor=False):\n",
        "    if active_user not in similarity_matrix.index:\n",
        "        print(f\"User {active_user} not found in the similarity matrix!\")\n",
        "        return {}\n",
        "\n",
        "    top_users = get_top_n_percent_pcc(similarity_matrix.loc[active_user], n=20)\n",
        "\n",
        "    if use_discount_factor:\n",
        "        # Apply discount factor\n",
        "        max_similarity = top_users.max()\n",
        "        discount_factor = 1 - (top_users / max_similarity)\n",
        "        discounted_similarity = top_users * (1 - discount_factor)\n",
        "    else:\n",
        "        discounted_similarity = top_users\n",
        "\n",
        "    # Predict ratings\n",
        "    predicted_ratings = {}\n",
        "\n",
        "    for item in user_item_matrix.columns:\n",
        "        if pd.notna(user_item_matrix.loc[active_user, item]):\n",
        "            continue  # Skip items already rated by the active user\n",
        "\n",
        "        # Weighted average of ratings from top users\n",
        "        numerator = (\n",
        "            (user_item_matrix[item] - mean_ratings[top_users.index]).fillna(0) * discounted_similarity\n",
        "        ).sum()\n",
        "        denominator = discounted_similarity.abs().sum()\n",
        "\n",
        "        # Add back the mean rating of the active user for prediction\n",
        "        predicted_ratings[item] = mean_ratings[active_user] + (numerator / denominator if denominator != 0 else 0)\n",
        "\n",
        "    return predicted_ratings\n",
        "\n",
        "# Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nAnalysis for {active_user}:\\n\")\n",
        "\n",
        "    # Step 1.3.3: Prediction without Discount Factor\n",
        "    predictions_no_df = predict_rating_pcc(\n",
        "        active_user, pcc_matrix, user_item_matrix, mean_ratings, use_discount_factor=False\n",
        "    )\n",
        "    print(\"Predictions without Discount Factor:\", predictions_no_df)\n",
        "\n",
        "    # Step 1.3.6: Prediction with Discount Factor\n",
        "    predictions_with_df = predict_rating_pcc(\n",
        "        active_user, pcc_matrix, user_item_matrix, mean_ratings, use_discount_factor=True\n",
        "    )\n",
        "    print(\"Predictions with Discount Factor:\", predictions_with_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdstrzbwhKpz",
        "outputId": "931b9920-b6d9-49df-a7e9-27903df4e348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analysis for User1:\n",
            "\n",
            "Predictions without Discount Factor: {'Come the Morning': 3.563034160074901, 'The Perfect Shadow': 2.569609063202309, \"Maybe It's True What They Say About Us\": 3.476992967950039, 'The Problem with People': 3.5098715599860033, 'Amityville: Where the Echo Lives': 3.767386732556104}\n",
            "Predictions with Discount Factor: {'Come the Morning': 3.501668777432018, 'The Perfect Shadow': 2.752866743277887, \"Maybe It's True What They Say About Us\": 3.298625719099718, 'The Problem with People': 3.363679011314331, 'Amityville: Where the Echo Lives': 3.5156401288909565}\n",
            "\n",
            "Analysis for User2:\n",
            "\n",
            "Predictions without Discount Factor: {'I giganti del cielo': 2.594560770334766, \"L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\": 3.1820411322438327, 'Power Alley': 3.2432267927853164}\n",
            "Predictions with Discount Factor: {'I giganti del cielo': 2.6749205413284627, \"L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\": 3.2186767516350128, 'Power Alley': 3.1796738354209713}\n",
            "\n",
            "Analysis for User3:\n",
            "\n",
            "Predictions without Discount Factor: {'Nine Ball': 3.916743085604992, 'Moe': 3.609559509055729}\n",
            "Predictions with Discount Factor: {'Nine Ball': 3.853369148786185, 'Moe': 3.4623352461892694}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the top 20% closest users to each active user."
      ],
      "metadata": {
        "id": "_UoDrhHAC9sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"  # Replace with your actual file path\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Mean-Centering: Subtract each user's mean rating from their ratings\n",
        "mean_ratings = user_item_matrix.mean(axis=1)\n",
        "mean_centered_matrix = user_item_matrix.sub(mean_ratings, axis=0)\n",
        "\n",
        "# Pearson Correlation Coefficient (PCC) calculation between users\n",
        "pcc_matrix = mean_centered_matrix.T.corr(method='pearson')  # Transpose to correlate users, not items\n",
        "\n",
        "# Function to find top N% closest users based on PCC\n",
        "def get_top_n_percent_pcc(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on PCC.\n",
        "    \"\"\"\n",
        "    # Sort users by similarity (highest first)\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nTop 20% Closest Users for {active_user} using PCC:\\n\")\n",
        "\n",
        "    # Get the similarity row for the active user\n",
        "    similarity_row = pcc_matrix.loc[active_user]\n",
        "\n",
        "    # Get the top 20% closest users based on PCC\n",
        "    top_20_percent_users = get_top_n_percent_pcc(similarity_row, n=20)\n",
        "\n",
        "    print(top_20_percent_users)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ6QZRWiC53s",
        "outputId": "2f2272bc-d185-4584-cdb5-6626b1070114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20% Closest Users for User1 using PCC:\n",
            "\n",
            "User1     1.000000\n",
            "User34    0.398041\n",
            "User10    0.351821\n",
            "User16    0.317940\n",
            "User9     0.281516\n",
            "User6     0.251077\n",
            "User5     0.182774\n",
            "User23    0.173773\n",
            "User29    0.163077\n",
            "User15    0.148594\n",
            "Name: User1, dtype: float64\n",
            "\n",
            "Top 20% Closest Users for User2 using PCC:\n",
            "\n",
            "User2     1.000000\n",
            "User21    0.522427\n",
            "User28    0.512616\n",
            "User38    0.483870\n",
            "User43    0.415225\n",
            "User45    0.329009\n",
            "User5     0.289938\n",
            "User25    0.264203\n",
            "User19    0.232580\n",
            "User39    0.208653\n",
            "Name: User2, dtype: float64\n",
            "\n",
            "Top 20% Closest Users for User3 using PCC:\n",
            "\n",
            "User3     1.000000\n",
            "User24    0.641427\n",
            "User26    0.305876\n",
            "User35    0.284052\n",
            "User12    0.278241\n",
            "User47    0.271917\n",
            "User42    0.241293\n",
            "User17    0.238366\n",
            "User22    0.202948\n",
            "User32    0.132131\n",
            "Name: User3, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the prediction for each active user to find whether the user will like or dislike the not yet seen or rated items.\n"
      ],
      "metadata": {
        "id": "no4LcofHDHGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"  # Replace with your actual file path\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Mean-Centering: Subtract each user's mean rating from their ratings\n",
        "mean_ratings = user_item_matrix.mean(axis=1)\n",
        "mean_centered_matrix = user_item_matrix.sub(mean_ratings, axis=0)\n",
        "\n",
        "# Pearson Correlation Coefficient (PCC) calculation between users\n",
        "pcc_matrix = mean_centered_matrix.T.corr(method='pearson')  # Transpose to correlate users, not items\n",
        "\n",
        "# Function to find top N% closest users based on PCC\n",
        "def get_top_n_percent_pcc(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on PCC.\n",
        "    \"\"\"\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Function to predict ratings for unrated items\n",
        "def predict_ratings(active_user, similarity_matrix, user_item_matrix, mean_ratings, n=20, threshold=3):\n",
        "    \"\"\"\n",
        "    Predict the ratings for items that the active user has not rated using PCC-based weighted average.\n",
        "    \"\"\"\n",
        "    if active_user not in similarity_matrix.index:\n",
        "        print(f\"User {active_user} not found in the similarity matrix!\")\n",
        "        return {}\n",
        "\n",
        "    # Get the similarity row for the active user\n",
        "    similarity_row = similarity_matrix.loc[active_user]\n",
        "\n",
        "    # Get the top N% closest users based on PCC\n",
        "    top_users = get_top_n_percent_pcc(similarity_row, n)\n",
        "\n",
        "    # Predict ratings for unrated items\n",
        "    predicted_ratings = {}\n",
        "\n",
        "    for item in user_item_matrix.columns:\n",
        "        if pd.notna(user_item_matrix.loc[active_user, item]):\n",
        "            continue  # Skip items already rated by the active user\n",
        "\n",
        "        # Calculate weighted average of ratings from top users\n",
        "        weighted_ratings = 0\n",
        "        similarity_sum = 0\n",
        "\n",
        "        for user in top_users.index:\n",
        "            if pd.notna(user_item_matrix.loc[user, item]):  # Only consider users who rated the item\n",
        "                weighted_ratings += similarity_row[user] * (user_item_matrix.loc[user, item] - mean_ratings[user])\n",
        "                similarity_sum += abs(similarity_row[user])\n",
        "\n",
        "        if similarity_sum != 0:\n",
        "            predicted_rating = mean_ratings[active_user] + (weighted_ratings / similarity_sum)\n",
        "        else:\n",
        "            predicted_rating = mean_ratings[active_user]  # Default to the user's mean rating if no similar users rated it\n",
        "\n",
        "        # Store the predicted rating for the item\n",
        "        predicted_ratings[item] = predicted_rating\n",
        "\n",
        "        # Determine whether the user will like or dislike the item\n",
        "        predicted_label = \"Like\" if predicted_rating >= threshold else \"Dislike\"\n",
        "        print(f\"Item: {item} | Predicted Rating: {predicted_rating:.2f} | Prediction: {predicted_label}\")\n",
        "\n",
        "    return predicted_ratings\n",
        "\n",
        "# Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nPrediction for {active_user}:\\n\")\n",
        "    predictions = predict_ratings(active_user, pcc_matrix, user_item_matrix, mean_ratings, n=20, threshold=3)\n",
        "    print(f\"Predicted Ratings for {active_user}: {predictions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9b3nUKRDNv_",
        "outputId": "ed5a9ec7-27d6-4ed2-f5ec-59722fc83397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prediction for User1:\n",
            "\n",
            "Item: Come the Morning | Predicted Rating: 3.72 | Prediction: Like\n",
            "Item: The Perfect Shadow | Predicted Rating: 2.16 | Prediction: Dislike\n",
            "Item: Maybe It's True What They Say About Us | Predicted Rating: 3.63 | Prediction: Like\n",
            "Item: The Problem with People | Predicted Rating: 3.68 | Prediction: Like\n",
            "Item: Amityville: Where the Echo Lives | Predicted Rating: 4.02 | Prediction: Like\n",
            "Predicted Ratings for User1: {'Come the Morning': 3.723058736689069, 'The Perfect Shadow': 2.163058343481172, \"Maybe It's True What They Say About Us\": 3.6340615246635566, 'The Problem with People': 3.6777551750413444, 'Amityville: Where the Echo Lives': 4.017489427120862}\n",
            "\n",
            "Prediction for User2:\n",
            "\n",
            "Item: I giganti del cielo | Predicted Rating: 2.42 | Prediction: Dislike\n",
            "Item: L'homme au bÃ¢ton, une lÃ©gende crÃ©ole | Predicted Rating: 3.20 | Prediction: Like\n",
            "Item: Power Alley | Predicted Rating: 3.29 | Prediction: Like\n",
            "Predicted Ratings for User2: {'I giganti del cielo': 2.424671697619767, \"L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\": 3.195472811540654, 'Power Alley': 3.290551327311407}\n",
            "\n",
            "Prediction for User3:\n",
            "\n",
            "Item: Nine Ball | Predicted Rating: 4.34 | Prediction: Like\n",
            "Item: Moe | Predicted Rating: 3.89 | Prediction: Like\n",
            "Predicted Ratings for User3: {'Nine Ball': 4.343507836579546, 'Moe': 3.885995618032799}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the discount factor (DF) then the discounted similarity (DS), for each of the active users considering the threshold in each case.\n"
      ],
      "metadata": {
        "id": "YEp7hnuZDG40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Mean-Centering: Subtract each user's mean rating from their ratings\n",
        "mean_ratings = user_item_matrix.mean(axis=1)\n",
        "mean_centered_matrix = user_item_matrix.sub(mean_ratings, axis=0)\n",
        "\n",
        "# Pearson Correlation Coefficient (PCC) calculation between users\n",
        "pcc_matrix = mean_centered_matrix.T.corr(method='pearson')  # Transpose to correlate users, not items\n",
        "\n",
        "# Function to find top N% closest users based on PCC\n",
        "def get_top_n_percent_pcc(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on PCC.\n",
        "    \"\"\"\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Function to calculate Discount Factor (DF) and Discounted Similarity (DS)\n",
        "def compute_discounted_similarity(active_user, similarity_matrix, n=20):\n",
        "    \"\"\"\n",
        "    Compute the Discount Factor (DF) and Discounted Similarity (DS) for each active user.\n",
        "    \"\"\"\n",
        "    if active_user not in similarity_matrix.index:\n",
        "        print(f\"User {active_user} not found in the similarity matrix!\")\n",
        "        return {}\n",
        "\n",
        "    # Get the similarity row for the active user\n",
        "    similarity_row = similarity_matrix.loc[active_user]\n",
        "\n",
        "    # Get the top N% closest users based on PCC\n",
        "    top_users = get_top_n_percent_pcc(similarity_row, n)\n",
        "\n",
        "    # Compute the Discount Factor (DF) and Discounted Similarity (DS)\n",
        "    max_similarity = top_users.max()  # Maximum similarity score for discounting\n",
        "\n",
        "    discounted_similarity = {}\n",
        "\n",
        "    for user, similarity in top_users.items():\n",
        "        # Compute DF (1 - (similarity / max_similarity))\n",
        "        df = 1 - (similarity / max_similarity)\n",
        "\n",
        "        # Compute DS (similarity * (1 - DF))\n",
        "        ds = similarity * (1 - df)\n",
        "\n",
        "        discounted_similarity[user] = {\n",
        "            \"DF\": df,\n",
        "            \"DS\": ds\n",
        "        }\n",
        "\n",
        "    return discounted_similarity\n",
        "\n",
        "# Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nDiscount Factor (DF) and Discounted Similarity (DS) for {active_user}:\\n\")\n",
        "    discounted_similarity = compute_discounted_similarity(active_user, pcc_matrix, n=20)\n",
        "\n",
        "    for user, values in discounted_similarity.items():\n",
        "        print(f\"User: {user} | DF: {values['DF']:.4f} | DS: {values['DS']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTbqXg2aDuH1",
        "outputId": "58443878-6c7c-4314-f8cb-e598d7870beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Discount Factor (DF) and Discounted Similarity (DS) for User1:\n",
            "\n",
            "User: User1 | DF: 0.0000 | DS: 1.0000\n",
            "User: User34 | DF: 0.6020 | DS: 0.1584\n",
            "User: User10 | DF: 0.6482 | DS: 0.1238\n",
            "User: User16 | DF: 0.6821 | DS: 0.1011\n",
            "User: User9 | DF: 0.7185 | DS: 0.0793\n",
            "User: User6 | DF: 0.7489 | DS: 0.0630\n",
            "User: User5 | DF: 0.8172 | DS: 0.0334\n",
            "User: User23 | DF: 0.8262 | DS: 0.0302\n",
            "User: User29 | DF: 0.8369 | DS: 0.0266\n",
            "User: User15 | DF: 0.8514 | DS: 0.0221\n",
            "\n",
            "Discount Factor (DF) and Discounted Similarity (DS) for User2:\n",
            "\n",
            "User: User2 | DF: 0.0000 | DS: 1.0000\n",
            "User: User21 | DF: 0.4776 | DS: 0.2729\n",
            "User: User28 | DF: 0.4874 | DS: 0.2628\n",
            "User: User38 | DF: 0.5161 | DS: 0.2341\n",
            "User: User43 | DF: 0.5848 | DS: 0.1724\n",
            "User: User45 | DF: 0.6710 | DS: 0.1082\n",
            "User: User5 | DF: 0.7101 | DS: 0.0841\n",
            "User: User25 | DF: 0.7358 | DS: 0.0698\n",
            "User: User19 | DF: 0.7674 | DS: 0.0541\n",
            "User: User39 | DF: 0.7913 | DS: 0.0435\n",
            "\n",
            "Discount Factor (DF) and Discounted Similarity (DS) for User3:\n",
            "\n",
            "User: User3 | DF: 0.0000 | DS: 1.0000\n",
            "User: User24 | DF: 0.3586 | DS: 0.4114\n",
            "User: User26 | DF: 0.6941 | DS: 0.0936\n",
            "User: User35 | DF: 0.7159 | DS: 0.0807\n",
            "User: User12 | DF: 0.7218 | DS: 0.0774\n",
            "User: User47 | DF: 0.7281 | DS: 0.0739\n",
            "User: User42 | DF: 0.7587 | DS: 0.0582\n",
            "User: User17 | DF: 0.7616 | DS: 0.0568\n",
            "User: User22 | DF: 0.7971 | DS: 0.0412\n",
            "User: User32 | DF: 0.8679 | DS: 0.0175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the top 20% closest users using Discounted Similarity\n"
      ],
      "metadata": {
        "id": "I9GqnfO9DGjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Mean-Centering: Subtract each user's mean rating from their ratings\n",
        "mean_ratings = user_item_matrix.mean(axis=1)\n",
        "mean_centered_matrix = user_item_matrix.sub(mean_ratings, axis=0)\n",
        "\n",
        "# Pearson Correlation Coefficient (PCC) calculation between users\n",
        "pcc_matrix = mean_centered_matrix.T.corr(method='pearson')  # Transpose to correlate users, not items\n",
        "\n",
        "# Function to find top N% closest users based on PCC\n",
        "def get_top_n_percent_pcc(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on PCC.\n",
        "    \"\"\"\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Function to calculate Discount Factor (DF) and Discounted Similarity (DS)\n",
        "def compute_discounted_similarity(active_user, similarity_matrix, n=20):\n",
        "    \"\"\"\n",
        "    Compute the Discount Factor (DF) and Discounted Similarity (DS) for each active user.\n",
        "    \"\"\"\n",
        "    if active_user not in similarity_matrix.index:\n",
        "        print(f\"User {active_user} not found in the similarity matrix!\")\n",
        "        return {}\n",
        "\n",
        "    # Get the similarity row for the active user\n",
        "    similarity_row = similarity_matrix.loc[active_user]\n",
        "\n",
        "    # Get the top N% closest users based on PCC\n",
        "    top_users = get_top_n_percent_pcc(similarity_row, n)\n",
        "\n",
        "    # Compute the Discount Factor (DF) and Discounted Similarity (DS)\n",
        "    max_similarity = top_users.max()  # Maximum similarity score for discounting\n",
        "\n",
        "    discounted_similarity = {}\n",
        "\n",
        "    for user, similarity in top_users.items():\n",
        "        # Compute DF (1 - (similarity / max_similarity))\n",
        "        df = 1 - (similarity / max_similarity)\n",
        "\n",
        "        # Compute DS (similarity * (1 - DF))\n",
        "        ds = similarity * (1 - df)\n",
        "\n",
        "        discounted_similarity[user] = {\n",
        "            \"DF\": df,\n",
        "            \"DS\": ds\n",
        "        }\n",
        "\n",
        "    return discounted_similarity\n",
        "\n",
        "# Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nTop 20% Closest Users for {active_user} based on Discounted Similarity:\\n\")\n",
        "\n",
        "    # Compute Discounted Similarity for the active user\n",
        "    discounted_similarity = compute_discounted_similarity(active_user, pcc_matrix, n=20)\n",
        "\n",
        "    # Sort users by their Discounted Similarity (DS) in descending order\n",
        "    sorted_discounted_similarity = sorted(discounted_similarity.items(), key=lambda x: x[1]['DS'], reverse=True)\n",
        "\n",
        "    # Print the top 20% closest users based on DS\n",
        "    for user, values in sorted_discounted_similarity:\n",
        "        print(f\"User: {user} | DF: {values['DF']:.4f} | DS: {values['DS']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "retZz0lQFAvv",
        "outputId": "9d6c55e5-f5fb-45e6-f13b-8e633424c930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20% Closest Users for User1 based on Discounted Similarity:\n",
            "\n",
            "User: User1 | DF: 0.0000 | DS: 1.0000\n",
            "User: User34 | DF: 0.6020 | DS: 0.1584\n",
            "User: User10 | DF: 0.6482 | DS: 0.1238\n",
            "User: User16 | DF: 0.6821 | DS: 0.1011\n",
            "User: User9 | DF: 0.7185 | DS: 0.0793\n",
            "User: User6 | DF: 0.7489 | DS: 0.0630\n",
            "User: User5 | DF: 0.8172 | DS: 0.0334\n",
            "User: User23 | DF: 0.8262 | DS: 0.0302\n",
            "User: User29 | DF: 0.8369 | DS: 0.0266\n",
            "User: User15 | DF: 0.8514 | DS: 0.0221\n",
            "\n",
            "Top 20% Closest Users for User2 based on Discounted Similarity:\n",
            "\n",
            "User: User2 | DF: 0.0000 | DS: 1.0000\n",
            "User: User21 | DF: 0.4776 | DS: 0.2729\n",
            "User: User28 | DF: 0.4874 | DS: 0.2628\n",
            "User: User38 | DF: 0.5161 | DS: 0.2341\n",
            "User: User43 | DF: 0.5848 | DS: 0.1724\n",
            "User: User45 | DF: 0.6710 | DS: 0.1082\n",
            "User: User5 | DF: 0.7101 | DS: 0.0841\n",
            "User: User25 | DF: 0.7358 | DS: 0.0698\n",
            "User: User19 | DF: 0.7674 | DS: 0.0541\n",
            "User: User39 | DF: 0.7913 | DS: 0.0435\n",
            "\n",
            "Top 20% Closest Users for User3 based on Discounted Similarity:\n",
            "\n",
            "User: User3 | DF: 0.0000 | DS: 1.0000\n",
            "User: User24 | DF: 0.3586 | DS: 0.4114\n",
            "User: User26 | DF: 0.6941 | DS: 0.0936\n",
            "User: User35 | DF: 0.7159 | DS: 0.0807\n",
            "User: User12 | DF: 0.7218 | DS: 0.0774\n",
            "User: User47 | DF: 0.7281 | DS: 0.0739\n",
            "User: User42 | DF: 0.7587 | DS: 0.0582\n",
            "User: User17 | DF: 0.7616 | DS: 0.0568\n",
            "User: User22 | DF: 0.7971 | DS: 0.0412\n",
            "User: User32 | DF: 0.8679 | DS: 0.0175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the prediction for each active user to find whether the user will like or dislike the not yet seen or rated items.\n"
      ],
      "metadata": {
        "id": "fFZznou1FpCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Mean-Centering: Subtract each user's mean rating from their ratings\n",
        "mean_ratings = user_item_matrix.mean(axis=1)\n",
        "mean_centered_matrix = user_item_matrix.sub(mean_ratings, axis=0)\n",
        "\n",
        "# Pearson Correlation Coefficient (PCC) calculation between users\n",
        "pcc_matrix = mean_centered_matrix.T.corr(method='pearson')  # Transpose to correlate users, not items\n",
        "\n",
        "# Function to find top N% closest users based on PCC\n",
        "def get_top_n_percent_pcc(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on PCC.\n",
        "    \"\"\"\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Function to calculate Discounted Similarity (DS)\n",
        "def compute_discounted_similarity(active_user, similarity_matrix, n=20):\n",
        "    \"\"\"\n",
        "    Compute the Discounted Similarity (DS) for each active user based on the Pearson Correlation.\n",
        "    \"\"\"\n",
        "    if active_user not in similarity_matrix.index:\n",
        "        print(f\"User {active_user} not found in the similarity matrix!\")\n",
        "        return {}\n",
        "\n",
        "    # Get the similarity row for the active user\n",
        "    similarity_row = similarity_matrix.loc[active_user]\n",
        "\n",
        "    # Get the top N% closest users based on PCC\n",
        "    top_users = get_top_n_percent_pcc(similarity_row, n)\n",
        "\n",
        "    # Compute the Discount Factor (DF) and Discounted Similarity (DS)\n",
        "    max_similarity = top_users.max()  # Maximum similarity score for discounting\n",
        "\n",
        "    discounted_similarity = {}\n",
        "\n",
        "    for user, similarity in top_users.items():\n",
        "        # Compute DF (1 - (similarity / max_similarity))\n",
        "        df = 1 - (similarity / max_similarity)\n",
        "\n",
        "        # Compute DS (similarity * (1 - DF))\n",
        "        ds = similarity * (1 - df)\n",
        "\n",
        "        discounted_similarity[user] = ds\n",
        "\n",
        "    return discounted_similarity\n",
        "\n",
        "# Function to predict ratings for unrated items\n",
        "def predict_ratings_for_unrated_items(active_user, user_item_matrix, similarity_matrix, mean_ratings, n=20):\n",
        "    \"\"\"\n",
        "    Predict ratings for unrated items for an active user based on Discounted Similarity (DS).\n",
        "    \"\"\"\n",
        "    # Get the top 20% closest users and their Discounted Similarity (DS)\n",
        "    discounted_similarity = compute_discounted_similarity(active_user, similarity_matrix, n)\n",
        "\n",
        "    # Initialize dictionary to store predictions\n",
        "    predicted_ratings = {}\n",
        "\n",
        "    # For each item that the active user has not rated\n",
        "    for item in user_item_matrix.columns:\n",
        "        if pd.isna(user_item_matrix.loc[active_user, item]):\n",
        "            # Compute the weighted sum of ratings for this item using DS\n",
        "            numerator = 0\n",
        "            denominator = 0\n",
        "\n",
        "            for user, ds in discounted_similarity.items():\n",
        "                if pd.notna(user_item_matrix.loc[user, item]):\n",
        "                    # Rating of the user for the item\n",
        "                    rating = user_item_matrix.loc[user, item]\n",
        "                    numerator += rating * ds\n",
        "                    denominator += abs(ds)\n",
        "\n",
        "            if denominator != 0:\n",
        "                predicted_ratings[item] = numerator / denominator + mean_ratings[active_user]\n",
        "            else:\n",
        "                predicted_ratings[item] = mean_ratings[active_user]  # If no rating available from top users\n",
        "\n",
        "    return predicted_ratings\n",
        "\n",
        "# Function to classify whether the user will like or dislike the item based on the predicted rating\n",
        "def classify_like_dislike(predicted_ratings, threshold=3):\n",
        "    \"\"\"\n",
        "    Classify whether the active user will like or dislike the item based on the predicted rating.\n",
        "    \"\"\"\n",
        "    like_dislike = {}\n",
        "    for item, predicted_rating in predicted_ratings.items():\n",
        "        like_dislike[item] = 'Like' if predicted_rating >= threshold else 'Dislike'\n",
        "    return like_dislike\n",
        "\n",
        "# Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nPrediction for {active_user}:\\n\")\n",
        "\n",
        "    # Step 1: Predict ratings for unrated items\n",
        "    predicted_ratings = predict_ratings_for_unrated_items(active_user, user_item_matrix, pcc_matrix, mean_ratings, n=20)\n",
        "    print(\"Predicted Ratings:\", predicted_ratings)\n",
        "\n",
        "    # Step 2: Classify as Like or Dislike based on threshold\n",
        "    like_dislike = classify_like_dislike(predicted_ratings, threshold=3)\n",
        "    print(\"Like/Dislike Classification:\", like_dislike)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEO6_KyZFqfV",
        "outputId": "1c41cea1-fd95-46f5-da45-7388d1f97ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prediction for User1:\n",
            "\n",
            "Predicted Ratings: {'Come the Morning': 6.977152605042631, 'The Perfect Shadow': 4.957398281123277, \"Maybe It's True What They Say About Us\": 6.442299223377886, 'The Problem with People': 6.644890942887692, 'Amityville: Where the Echo Lives': 7.013027085674745}\n",
            "Like/Dislike Classification: {'Come the Morning': 'Like', 'The Perfect Shadow': 'Like', \"Maybe It's True What They Say About Us\": 'Like', 'The Problem with People': 'Like', 'Amityville: Where the Echo Lives': 'Like'}\n",
            "\n",
            "Prediction for User2:\n",
            "\n",
            "Predicted Ratings: {'I giganti del cielo': 5.457458082562392, \"L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\": 6.428213211982612, 'Power Alley': 6.319628054316487}\n",
            "Like/Dislike Classification: {'I giganti del cielo': 'Like', \"L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\": 'Like', 'Power Alley': 'Like'}\n",
            "\n",
            "Prediction for User3:\n",
            "\n",
            "Predicted Ratings: {'Nine Ball': 7.838639893165304, 'Moe': 6.952356245999453}\n",
            "Like/Dislike Classification: {'Nine Ball': 'Like', 'Moe': 'Like'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 2**\n",
        "**Case Study 2.1**"
      ],
      "metadata": {
        "id": "t7XEHMHli-Jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Item-based Collaborative Filtering algorithms using Cosine similarity\n",
        "without considering the bias adjustment effect of mean-centering"
      ],
      "metadata": {
        "id": "inTLa0WYjV-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(\"user_item_ratings.csv\", index_col=0)\n",
        "\n",
        "# Handle missing values (NaN) by filling with 0 or using another method\n",
        "data_filled = data.fillna(0)\n",
        "\n",
        "# Step 1: Cosine Similarity Calculation\n",
        "# Compute the cosine similarity matrix between items\n",
        "cosine_sim = cosine_similarity(data_filled)\n",
        "\n",
        "# Create a DataFrame for the similarity matrix\n",
        "cosine_sim_df = pd.DataFrame(cosine_sim, index=data.index, columns=data.index)\n",
        "\n",
        "# Step 2: Predict Ratings without Discount Factor\n",
        "def predict_rating_no_df(item, user):\n",
        "    # Get the top 25% most similar items for the target item\n",
        "    similar_items = cosine_sim_df[item].sort_values(ascending=False).head(int(len(cosine_sim_df) * 0.25))\n",
        "\n",
        "    # Compute the weighted average of ratings based on similarity\n",
        "    weighted_ratings = 0\n",
        "    similarity_sum = 0\n",
        "    for similar_item, similarity in similar_items.items():\n",
        "        rating = data.loc[similar_item, user]\n",
        "        if rating > 0:  # Only consider non-zero ratings\n",
        "            weighted_ratings += similarity * rating\n",
        "            similarity_sum += similarity\n",
        "\n",
        "    if similarity_sum == 0:  # Prevent division by zero\n",
        "        return 0\n",
        "    return weighted_ratings / similarity_sum\n",
        "\n",
        "# Step 3: Apply Discount Factor and Predict Ratings\n",
        "def predict_rating_with_df(item, user, discount_factor_func=None):\n",
        "    # Get the top 20% most similar items for the target item\n",
        "    similar_items = cosine_sim_df[item].sort_values(ascending=False).head(int(len(cosine_sim_df) * 0.2))\n",
        "\n",
        "    # Compute the discounted similarity\n",
        "    weighted_ratings = 0\n",
        "    similarity_sum = 0\n",
        "    for similar_item, similarity in similar_items.items():\n",
        "        rating = data.loc[similar_item, user]\n",
        "        if rating > 0:  # Only consider non-zero ratings\n",
        "            # Apply the discount factor if provided\n",
        "            discount_factor = discount_factor_func(similarity) if discount_factor_func else 1\n",
        "            weighted_ratings += similarity * discount_factor * rating\n",
        "            similarity_sum += similarity * discount_factor\n",
        "\n",
        "    if similarity_sum == 0:\n",
        "        return 0\n",
        "    return weighted_ratings / similarity_sum\n",
        "\n",
        "# Example discount factor function (based on cosine similarity)\n",
        "def discount_factor(similarity):\n",
        "    return 1 / (1 + np.exp(-similarity))  # Logistic function to reduce impact of less similar items\n",
        "\n",
        "# Step 4: Loop through Active Users and Predict Missing Ratings\n",
        "active_users = ['User1', 'User2', 'User3']\n",
        "target_items = data.index  # All items for which we need to predict ratings\n",
        "\n",
        "# For each active user, calculate predictions only for missing values\n",
        "for user in active_users:\n",
        "    print(f\"\\nPredictions for {user}:\")\n",
        "    for item in target_items:\n",
        "        if pd.isna(data.loc[item, user]):  # Only predict for missing values\n",
        "            # Prediction without discount factor\n",
        "            pred_no_df = predict_rating_no_df(item, user)\n",
        "            # Prediction with discount factor\n",
        "            pred_with_df = predict_rating_with_df(item, user, discount_factor)\n",
        "\n",
        "            print(f\"Item: {item}\")\n",
        "            print(f\"Predicted Rating without Discount Factor: {pred_no_df}\")\n",
        "            print(f\"Predicted Rating with Discount Factor: {pred_with_df}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Acxr-27LtouL",
        "outputId": "494ea85f-e70d-4f44-80f9-1302082616f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions for User1:\n",
            "Item: Come the Morning\n",
            "Predicted Rating without Discount Factor: 3.3442989241961705\n",
            "Predicted Rating with Discount Factor: 3.2209147475720004\n",
            "Item: The Perfect Shadow\n",
            "Predicted Rating without Discount Factor: 3.1709818192083374\n",
            "Predicted Rating with Discount Factor: 3.4027915502152233\n",
            "Item: Maybe It's True What They Say About Us\n",
            "Predicted Rating without Discount Factor: 3.488525444537169\n",
            "Predicted Rating with Discount Factor: 2.9893840625035826\n",
            "Item: The Problem with People\n",
            "Predicted Rating without Discount Factor: 2.6112110206643826\n",
            "Predicted Rating with Discount Factor: 2.6139776551235494\n",
            "Item: Amityville: Where the Echo Lives\n",
            "Predicted Rating without Discount Factor: 2.506550729394105\n",
            "Predicted Rating with Discount Factor: 2.352830428004009\n",
            "\n",
            "Predictions for User2:\n",
            "Item: I giganti del cielo\n",
            "Predicted Rating without Discount Factor: 2.5983193150108006\n",
            "Predicted Rating with Discount Factor: 2.501453363057643\n",
            "Item: L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\n",
            "Predicted Rating without Discount Factor: 3.585998415171674\n",
            "Predicted Rating with Discount Factor: 3.239250890065275\n",
            "Item: Power Alley\n",
            "Predicted Rating without Discount Factor: 2.7828408708791965\n",
            "Predicted Rating with Discount Factor: 2.2444684933095513\n",
            "\n",
            "Predictions for User3:\n",
            "Item: Nine Ball\n",
            "Predicted Rating without Discount Factor: 2.9758569932752286\n",
            "Predicted Rating with Discount Factor: 2.7738947593033867\n",
            "Item: Moe\n",
            "Predicted Rating without Discount Factor: 2.832466736962769\n",
            "Predicted Rating with Discount Factor: 2.9946003383835946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the top 20% closest users to each active user."
      ],
      "metadata": {
        "id": "Ru9u7SecHGux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(\"user_item_ratings.csv\", index_col=0)\n",
        "\n",
        "# Handle missing values (NaN) by filling with 0 or using another method\n",
        "data_filled = data.fillna(0)\n",
        "\n",
        "# Step 1: Cosine Similarity Calculation Between Users\n",
        "# Compute the cosine similarity matrix between users\n",
        "cosine_sim_users = cosine_similarity(data_filled.T)  # Transpose to compute similarity between users\n",
        "\n",
        "# Create a DataFrame for the similarity matrix (between users)\n",
        "cosine_sim_users_df = pd.DataFrame(cosine_sim_users, index=data.columns, columns=data.columns)\n",
        "\n",
        "# Step 2: Get Top 20% Closest Users for Each Active User\n",
        "def get_top_n_percent_users(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on Cosine Similarity.\n",
        "    \"\"\"\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Step 3: Loop through Active Users and Get Top 20% Closest Users\n",
        "active_users = ['User1', 'User2', 'User3']  # List of active users\n",
        "for user in active_users:\n",
        "    print(f\"\\nTop 20% Closest Users to {user}:\")\n",
        "\n",
        "    # Get the similarity row for the active user\n",
        "    similarity_row = cosine_sim_users_df[user]\n",
        "\n",
        "    # Get the top 20% closest users based on Cosine Similarity\n",
        "    top_users = get_top_n_percent_users(similarity_row, n=20)\n",
        "\n",
        "    # Display the top 20% closest users\n",
        "    print(top_users)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6grz8dh1GiuS",
        "outputId": "e60503ba-539d-4ac4-a257-431316428a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20% Closest Users to User1:\n",
            "User1     1.000000\n",
            "User16    0.811344\n",
            "User5     0.805661\n",
            "User25    0.796926\n",
            "User4     0.793058\n",
            "User34    0.790270\n",
            "User49    0.789581\n",
            "User38    0.773793\n",
            "User46    0.764264\n",
            "User17    0.764253\n",
            "Name: User1, dtype: float64\n",
            "\n",
            "Top 20% Closest Users to User2:\n",
            "User2     1.000000\n",
            "User28    0.886142\n",
            "User38    0.840459\n",
            "User45    0.830957\n",
            "User25    0.821160\n",
            "User19    0.817527\n",
            "User39    0.815440\n",
            "User21    0.799950\n",
            "User12    0.799177\n",
            "User23    0.791842\n",
            "Name: User2, dtype: float64\n",
            "\n",
            "Top 20% Closest Users to User3:\n",
            "User3     1.000000\n",
            "User29    0.861920\n",
            "User17    0.844031\n",
            "User23    0.830241\n",
            "User22    0.829816\n",
            "User35    0.828374\n",
            "User12    0.828334\n",
            "User14    0.828183\n",
            "User25    0.826934\n",
            "User6     0.825333\n",
            "Name: User3, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the prediction for each active user to find whether the user will like or dislike the not yet seen or rated items."
      ],
      "metadata": {
        "id": "TqbrQPowHF5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(\"user_item_ratings.csv\", index_col=0)\n",
        "\n",
        "# Handle missing values (NaN) by filling with 0 or using another method\n",
        "data_filled = data.fillna(0)\n",
        "\n",
        "# Step 1: Cosine Similarity Calculation Between Users\n",
        "# Compute the cosine similarity matrix between users\n",
        "cosine_sim_users = cosine_similarity(data_filled.T)  # Transpose to compute similarity between users\n",
        "\n",
        "# Create a DataFrame for the similarity matrix (between users)\n",
        "cosine_sim_users_df = pd.DataFrame(cosine_sim_users, index=data.columns, columns=data.columns)\n",
        "\n",
        "# Step 2: Get Top 20% Closest Users for Each Active User\n",
        "def get_top_n_percent_users(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on Cosine Similarity.\n",
        "    \"\"\"\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Step 3: Predict Rating for Missing Items\n",
        "def predict_rating_for_item(item, user, top_users, similarity_matrix, ratings_matrix):\n",
        "    \"\"\"\n",
        "    Predict the rating for a missing item based on top N% closest users.\n",
        "    \"\"\"\n",
        "    weighted_ratings = 0\n",
        "    similarity_sum = 0\n",
        "    for similar_user, similarity in top_users.items():\n",
        "        rating = ratings_matrix.loc[item, similar_user]\n",
        "        if rating > 0:  # Only consider non-zero ratings\n",
        "            weighted_ratings += similarity * rating\n",
        "            similarity_sum += similarity\n",
        "\n",
        "    if similarity_sum == 0:\n",
        "        return 0  # If no similar ratings, return 0\n",
        "    return weighted_ratings / similarity_sum\n",
        "\n",
        "# Step 4: Loop through Active Users and Predict Missing Ratings\n",
        "active_users = ['User1', 'User2', 'User3']  # List of active users\n",
        "target_items = data.index  # All items for which we need to predict ratings\n",
        "\n",
        "# For each active user, calculate predictions only for missing values\n",
        "for user in active_users:\n",
        "    print(f\"\\nPredictions for {user}:\")\n",
        "\n",
        "    # Get the similarity row for the active user\n",
        "    similarity_row = cosine_sim_users_df[user]\n",
        "\n",
        "    # Get the top 20% closest users based on Cosine Similarity\n",
        "    top_users = get_top_n_percent_users(similarity_row, n=20)\n",
        "\n",
        "    for item in target_items:\n",
        "        if pd.isna(data.loc[item, user]):  # Only predict for missing values\n",
        "            # Predict the rating for the missing item\n",
        "            predicted_rating = predict_rating_for_item(item, user, top_users, cosine_sim_users_df, data)\n",
        "\n",
        "            # Assume a rating threshold of 3 to determine if the user will like or dislike the item\n",
        "            like_or_dislike = \"Like\" if predicted_rating >= 3 else \"Dislike\"\n",
        "\n",
        "            print(f\"Item: {item}\")\n",
        "            print(f\"Predicted Rating: {predicted_rating}\")\n",
        "            print(f\"Prediction: {like_or_dislike}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2YtJp04GiZO",
        "outputId": "81fbc861-b453-4402-b97d-e31ae9871878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions for User1:\n",
            "Item: Come the Morning\n",
            "Predicted Rating: 2.891997846821858\n",
            "Prediction: Dislike\n",
            "Item: The Perfect Shadow\n",
            "Predicted Rating: 2.865730574613695\n",
            "Prediction: Dislike\n",
            "Item: Maybe It's True What They Say About Us\n",
            "Predicted Rating: 2.3828517785659646\n",
            "Prediction: Dislike\n",
            "Item: The Problem with People\n",
            "Predicted Rating: 2.6875030272577356\n",
            "Prediction: Dislike\n",
            "Item: Amityville: Where the Echo Lives\n",
            "Predicted Rating: 3.4937522567638335\n",
            "Prediction: Like\n",
            "\n",
            "Predictions for User2:\n",
            "Item: I giganti del cielo\n",
            "Predicted Rating: 2.3237463214575764\n",
            "Prediction: Dislike\n",
            "Item: L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\n",
            "Predicted Rating: 2.763063324327931\n",
            "Prediction: Dislike\n",
            "Item: Power Alley\n",
            "Predicted Rating: 3.2233434837529793\n",
            "Prediction: Like\n",
            "\n",
            "Predictions for User3:\n",
            "Item: Nine Ball\n",
            "Predicted Rating: 2.6667464480208083\n",
            "Prediction: Dislike\n",
            "Item: Moe\n",
            "Predicted Rating: 3.703449391010854\n",
            "Prediction: Like\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the discount factor (DF) then the discounted similarity (DS), for each of the active users considering the threshold in each case."
      ],
      "metadata": {
        "id": "D7ySDXGNHg8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(\"user_item_ratings.csv\", index_col=0)\n",
        "\n",
        "# Handle missing values (NaN) by filling with 0 or using another method\n",
        "data_filled = data.fillna(0)\n",
        "\n",
        "# Step 1: Cosine Similarity Calculation Between Users\n",
        "# Compute the cosine similarity matrix between users\n",
        "cosine_sim_users = cosine_similarity(data_filled.T)  # Transpose to compute similarity between users\n",
        "\n",
        "# Create a DataFrame for the similarity matrix (between users)\n",
        "cosine_sim_users_df = pd.DataFrame(cosine_sim_users, index=data.columns, columns=data.columns)\n",
        "\n",
        "# Step 2: Discount Factor (DF) - using a logistic function for demonstration\n",
        "def discount_factor(similarity, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Compute the discount factor based on the cosine similarity.\n",
        "    The closer the similarity is to 1, the smaller the discount.\n",
        "    \"\"\"\n",
        "    if similarity < threshold:\n",
        "        return 1  # If similarity is below threshold, no discount\n",
        "    return 1 / (1 + np.exp(-similarity))  # Logistic function\n",
        "\n",
        "# Step 3: Compute Discounted Similarity (DS) for Each Active User\n",
        "def compute_discounted_similarity(user, top_users, similarity_matrix):\n",
        "    \"\"\"\n",
        "    For each active user, compute the discounted similarity (DS) for each of the top N% closest users.\n",
        "    \"\"\"\n",
        "    discounted_similarities = {}\n",
        "    for similar_user, similarity in top_users.items():\n",
        "        # Calculate the discount factor based on the similarity\n",
        "        df = discount_factor(similarity)\n",
        "        # Compute the discounted similarity\n",
        "        ds = similarity * df\n",
        "        discounted_similarities[similar_user] = ds\n",
        "    return discounted_similarities\n",
        "\n",
        "# Step 4: Loop through Active Users and Compute Discounted Similarities\n",
        "active_users = ['User1', 'User2', 'User3']  # List of active users\n",
        "target_items = data.index  # All items for which we need to predict ratings\n",
        "\n",
        "# For each active user, calculate predictions only for missing values\n",
        "for user in active_users:\n",
        "    print(f\"\\nDiscounted Similarities for {user}:\")\n",
        "\n",
        "    # Get the similarity row for the active user\n",
        "    similarity_row = cosine_sim_users_df[user]\n",
        "\n",
        "    # Get the top 20% closest users based on Cosine Similarity\n",
        "    top_users = get_top_n_percent_users(similarity_row, n=20)\n",
        "\n",
        "    # Compute the discounted similarities\n",
        "    discounted_similarities = compute_discounted_similarity(user, top_users, cosine_sim_users_df)\n",
        "\n",
        "    # Output the discounted similarities\n",
        "    for similar_user, ds in discounted_similarities.items():\n",
        "        print(f\"User: {similar_user}, Discounted Similarity (DS): {ds}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMUBpJuFHoKj",
        "outputId": "0b89a75f-f7a9-4797-d580-cc9c15c52002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Discounted Similarities for User1:\n",
            "User: User1, Discounted Similarity (DS): 0.731058578630005\n",
            "User: User16, Discounted Similarity (DS): 0.5617711540156594\n",
            "User: User5, Discounted Similarity (DS): 0.5568600450647789\n",
            "User: User25, Discounted Similarity (DS): 0.5493340975285731\n",
            "User: User4, Discounted Similarity (DS): 0.5460105552713351\n",
            "User: User34, Discounted Similarity (DS): 0.543618391895568\n",
            "User: User49, Discounted Similarity (DS): 0.5430274398122475\n",
            "User: User38, Discounted Similarity (DS): 0.5295377789216732\n",
            "User: User46, Discounted Similarity (DS): 0.5214408347931581\n",
            "User: User17, Discounted Similarity (DS): 0.521431695935111\n",
            "\n",
            "Discounted Similarities for User2:\n",
            "User: User2, Discounted Similarity (DS): 0.7310585786300049\n",
            "User: User28, Discounted Similarity (DS): 0.6274715075042898\n",
            "User: User38, Discounted Similarity (DS): 0.5871125446534223\n",
            "User: User45, Discounted Similarity (DS): 0.5788087192283734\n",
            "User: User25, Discounted Similarity (DS): 0.5702810866054778\n",
            "User: User19, Discounted Similarity (DS): 0.5671274501256415\n",
            "User: User39, Discounted Similarity (DS): 0.5653184618377859\n",
            "User: User21, Discounted Similarity (DS): 0.5519361792739897\n",
            "User: User12, Discounted Similarity (DS): 0.5512711064281924\n",
            "User: User23, Discounted Similarity (DS): 0.5449670232945546\n",
            "\n",
            "Discounted Similarities for User3:\n",
            "User: User3, Discounted Similarity (DS): 0.731058578630005\n",
            "User: User29, Discounted Similarity (DS): 0.6059824638387229\n",
            "User: User17, Discounted Similarity (DS): 0.5902427050012947\n",
            "User: User23, Discounted Similarity (DS): 0.5781850742332537\n",
            "User: User22, Discounted Similarity (DS): 0.57781430997939\n",
            "User: User35, Discounted Similarity (DS): 0.576557644328422\n",
            "User: User12, Discounted Similarity (DS): 0.576522838160114\n",
            "User: User14, Discounted Similarity (DS): 0.5763912772407452\n",
            "User: User25, Discounted Similarity (DS): 0.5753032435764935\n",
            "User: User6, Discounted Similarity (DS): 0.5739095839688126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the top 20% closest users using Discounted Similarity\n"
      ],
      "metadata": {
        "id": "4JfcQeSHHk0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(\"user_item_ratings.csv\", index_col=0)\n",
        "\n",
        "# Handle missing values (NaN) by filling with 0\n",
        "data_filled = data.fillna(0)\n",
        "\n",
        "# Step 1: Compute Cosine Similarity between Users\n",
        "cosine_sim_users = cosine_similarity(data_filled.T)  # Compute similarity between users (transposed matrix)\n",
        "cosine_sim_users_df = pd.DataFrame(cosine_sim_users, index=data.columns, columns=data.columns)\n",
        "\n",
        "# Step 2: Define Discount Factor (DF) using a Logistic Function\n",
        "def discount_factor(similarity, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Compute the discount factor based on the cosine similarity.\n",
        "    The closer the similarity is to 1, the smaller the discount.\n",
        "    \"\"\"\n",
        "    if similarity < threshold:\n",
        "        return 1  # If similarity is below threshold, no discount\n",
        "    return 1 / (1 + np.exp(-similarity))  # Logistic function\n",
        "\n",
        "# Step 3: Compute Discounted Similarity (DS) for Each Active User\n",
        "def compute_discounted_similarity(user, top_users, similarity_matrix):\n",
        "    \"\"\"\n",
        "    Compute discounted similarities (DS) for each of the top N% closest users.\n",
        "    \"\"\"\n",
        "    discounted_similarities = {}\n",
        "    for similar_user, similarity in top_users.items():\n",
        "        # Calculate the discount factor based on the similarity\n",
        "        df = discount_factor(similarity)\n",
        "        # Compute the discounted similarity\n",
        "        ds = similarity * df\n",
        "        discounted_similarities[similar_user] = ds\n",
        "    return discounted_similarities\n",
        "\n",
        "# Step 4: Get Top N% Closest Users Based on Discounted Similarity\n",
        "def get_top_n_percent_users(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on similarity.\n",
        "    \"\"\"\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)  # Sort users by similarity (highest first)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Step 5: Loop Through Active Users and Determine Top 20% Closest Users\n",
        "active_users = ['User1', 'User2', 'User3']  # List of active users\n",
        "\n",
        "# For each active user, calculate the top 20% closest users based on discounted similarity\n",
        "for user in active_users:\n",
        "    print(f\"\\nTop 20% Closest Users for {user} based on Discounted Similarity:\")\n",
        "\n",
        "    # Get the similarity row for the active user\n",
        "    similarity_row = cosine_sim_users_df[user]\n",
        "\n",
        "    # Get the top 20% closest users based on Cosine Similarity\n",
        "    top_users = get_top_n_percent_users(similarity_row, n=20)\n",
        "\n",
        "    # Compute the discounted similarities for the top users\n",
        "    discounted_similarities = compute_discounted_similarity(user, top_users, cosine_sim_users_df)\n",
        "\n",
        "    # Sort the discounted similarities in descending order to find the top users\n",
        "    sorted_discounted_similarities = sorted(discounted_similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Display the top 20% closest users based on discounted similarity\n",
        "    for similar_user, ds in sorted_discounted_similarities:\n",
        "        print(f\"User: {similar_user}, Discounted Similarity (DS): {ds}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx18FLFnHnz-",
        "outputId": "80eb40e8-7318-4ea8-fb76-2498f17393d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20% Closest Users for User1 based on Discounted Similarity:\n",
            "User: User1, Discounted Similarity (DS): 0.731058578630005\n",
            "User: User16, Discounted Similarity (DS): 0.5617711540156594\n",
            "User: User5, Discounted Similarity (DS): 0.5568600450647789\n",
            "User: User25, Discounted Similarity (DS): 0.5493340975285731\n",
            "User: User4, Discounted Similarity (DS): 0.5460105552713351\n",
            "User: User34, Discounted Similarity (DS): 0.543618391895568\n",
            "User: User49, Discounted Similarity (DS): 0.5430274398122475\n",
            "User: User38, Discounted Similarity (DS): 0.5295377789216732\n",
            "User: User46, Discounted Similarity (DS): 0.5214408347931581\n",
            "User: User17, Discounted Similarity (DS): 0.521431695935111\n",
            "\n",
            "Top 20% Closest Users for User2 based on Discounted Similarity:\n",
            "User: User2, Discounted Similarity (DS): 0.7310585786300049\n",
            "User: User28, Discounted Similarity (DS): 0.6274715075042898\n",
            "User: User38, Discounted Similarity (DS): 0.5871125446534223\n",
            "User: User45, Discounted Similarity (DS): 0.5788087192283734\n",
            "User: User25, Discounted Similarity (DS): 0.5702810866054778\n",
            "User: User19, Discounted Similarity (DS): 0.5671274501256415\n",
            "User: User39, Discounted Similarity (DS): 0.5653184618377859\n",
            "User: User21, Discounted Similarity (DS): 0.5519361792739897\n",
            "User: User12, Discounted Similarity (DS): 0.5512711064281924\n",
            "User: User23, Discounted Similarity (DS): 0.5449670232945546\n",
            "\n",
            "Top 20% Closest Users for User3 based on Discounted Similarity:\n",
            "User: User3, Discounted Similarity (DS): 0.731058578630005\n",
            "User: User29, Discounted Similarity (DS): 0.6059824638387229\n",
            "User: User17, Discounted Similarity (DS): 0.5902427050012947\n",
            "User: User23, Discounted Similarity (DS): 0.5781850742332537\n",
            "User: User22, Discounted Similarity (DS): 0.57781430997939\n",
            "User: User35, Discounted Similarity (DS): 0.576557644328422\n",
            "User: User12, Discounted Similarity (DS): 0.576522838160114\n",
            "User: User14, Discounted Similarity (DS): 0.5763912772407452\n",
            "User: User25, Discounted Similarity (DS): 0.5753032435764935\n",
            "User: User6, Discounted Similarity (DS): 0.5739095839688126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the prediction for each active user to find whether the user will like or dislike the not yet seen or rated items.\n"
      ],
      "metadata": {
        "id": "K6WczpVlHmuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(\"user_item_ratings.csv\", index_col=0)\n",
        "\n",
        "# Handle missing values (NaN) by filling with 0\n",
        "data_filled = data.fillna(0)\n",
        "\n",
        "# Step 1: Compute Cosine Similarity between Users\n",
        "cosine_sim_users = cosine_similarity(data_filled.T)  # Compute similarity between users (transposed matrix)\n",
        "cosine_sim_users_df = pd.DataFrame(cosine_sim_users, index=data.columns, columns=data.columns)\n",
        "\n",
        "# Step 2: Define Discount Factor (DF) using a Logistic Function\n",
        "def discount_factor(similarity, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Compute the discount factor based on the cosine similarity.\n",
        "    The closer the similarity is to 1, the smaller the discount.\n",
        "    \"\"\"\n",
        "    if similarity < threshold:\n",
        "        return 1  # If similarity is below threshold, no discount\n",
        "    return 1 / (1 + np.exp(-similarity))  # Logistic function\n",
        "\n",
        "# Step 3: Compute Discounted Similarity (DS) for Each Active User\n",
        "def compute_discounted_similarity(user, top_users, similarity_matrix):\n",
        "    \"\"\"\n",
        "    Compute discounted similarities (DS) for each of the top N% closest users.\n",
        "    \"\"\"\n",
        "    discounted_similarities = {}\n",
        "    for similar_user, similarity in top_users.items():\n",
        "        # Calculate the discount factor based on the similarity\n",
        "        df = discount_factor(similarity)\n",
        "        # Compute the discounted similarity\n",
        "        ds = similarity * df\n",
        "        discounted_similarities[similar_user] = ds\n",
        "    return discounted_similarities\n",
        "\n",
        "# Step 4: Get Top N% Closest Users Based on Discounted Similarity\n",
        "def get_top_n_percent_users(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on similarity.\n",
        "    \"\"\"\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)  # Sort users by similarity (highest first)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Step 5: Compute Predictions for Missing Ratings\n",
        "def predict_rating(user, item, top_users, cosine_sim_users_df, data):\n",
        "    \"\"\"\n",
        "    Predict the rating for an item that the user hasn't rated based on discounted similarity.\n",
        "    \"\"\"\n",
        "    discounted_similarities = compute_discounted_similarity(user, top_users, cosine_sim_users_df)\n",
        "\n",
        "    weighted_ratings = 0\n",
        "    similarity_sum = 0\n",
        "    for similar_user, ds in discounted_similarities.items():\n",
        "        rating = data.loc[item, similar_user]  # Get the rating of the similar user for the item\n",
        "        if rating > 0:  # Only consider users who have rated the item\n",
        "            weighted_ratings += ds * rating\n",
        "            similarity_sum += ds\n",
        "\n",
        "    # Prevent division by zero\n",
        "    if similarity_sum == 0:\n",
        "        return 0\n",
        "\n",
        "    # Return the weighted average rating prediction\n",
        "    return weighted_ratings / similarity_sum\n",
        "\n",
        "# Step 6: Determine Like or Dislike Based on Prediction\n",
        "def determine_like_or_dislike(predicted_rating, threshold=3.5):\n",
        "    \"\"\"\n",
        "    Determine whether the user will like or dislike the item based on the predicted rating.\n",
        "    \"\"\"\n",
        "    return \"Like\" if predicted_rating >= threshold else \"Dislike\"\n",
        "\n",
        "# Step 7: Loop Through Active Users and Predict Missing Ratings\n",
        "active_users = ['User1', 'User2', 'User3']  # List of active users\n",
        "threshold = 3.5  # Threshold for liking/disliking an item\n",
        "\n",
        "# For each active user, calculate predictions only for missing values\n",
        "for user in active_users:\n",
        "    print(f\"\\nPredictions for {user}:\")\n",
        "\n",
        "    # Get the similarity row for the active user\n",
        "    similarity_row = cosine_sim_users_df[user]\n",
        "\n",
        "    # Get the top 20% closest users based on Cosine Similarity\n",
        "    top_users = get_top_n_percent_users(similarity_row, n=20)\n",
        "\n",
        "    # Predict missing ratings for all items\n",
        "    for item in data.index:\n",
        "        if pd.isna(data.loc[item, user]):  # Only predict for missing values\n",
        "            # Predict the rating for the missing item\n",
        "            predicted_rating = predict_rating(user, item, top_users, cosine_sim_users_df, data)\n",
        "\n",
        "            # Determine whether the user will like or dislike the item\n",
        "            like_or_dislike = determine_like_or_dislike(predicted_rating, threshold)\n",
        "\n",
        "            print(f\"Item: {item}, Predicted Rating: {predicted_rating:.2f}, Predicted: {like_or_dislike}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig2_HwvWHnej",
        "outputId": "6b5c588b-5af0-4881-ffc0-10241b1935a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions for User1:\n",
            "Item: Come the Morning, Predicted Rating: 2.89, Predicted: Dislike\n",
            "Item: The Perfect Shadow, Predicted Rating: 2.86, Predicted: Dislike\n",
            "Item: Maybe It's True What They Say About Us, Predicted Rating: 2.38, Predicted: Dislike\n",
            "Item: The Problem with People, Predicted Rating: 2.69, Predicted: Dislike\n",
            "Item: Amityville: Where the Echo Lives, Predicted Rating: 3.49, Predicted: Dislike\n",
            "\n",
            "Predictions for User2:\n",
            "Item: I giganti del cielo, Predicted Rating: 2.32, Predicted: Dislike\n",
            "Item: L'homme au bÃ¢ton, une lÃ©gende crÃ©ole, Predicted Rating: 2.77, Predicted: Dislike\n",
            "Item: Power Alley, Predicted Rating: 3.22, Predicted: Dislike\n",
            "\n",
            "Predictions for User3:\n",
            "Item: Nine Ball, Predicted Rating: 2.67, Predicted: Dislike\n",
            "Item: Moe, Predicted Rating: 3.70, Predicted: Like\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case Study 2.2**"
      ],
      "metadata": {
        "id": "aWjNkk05uLDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Item-based Collaborative Filtering algorithms using Cosine Similarity\n",
        "considering the bias adjustment effect of mean-centering"
      ],
      "metadata": {
        "id": "yZtkv0RbuPaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the data (assumed to be CSV format with users as columns and items as rows)\n",
        "data = pd.read_csv(\"user_item_ratings.csv\", index_col=0)\n",
        "\n",
        "# Step 1: Mean-Centering\n",
        "# Calculate the mean rating for each item\n",
        "mean_ratings = data.mean(axis=1)\n",
        "\n",
        "# Subtract the mean rating from each item's ratings to center them\n",
        "data_centered = data.sub(mean_ratings, axis=0)\n",
        "\n",
        "# Step 2: Cosine Similarity with Mean-Centering\n",
        "# Compute the cosine similarity between items based on the centered ratings\n",
        "cosine_sim = cosine_similarity(data_centered.fillna(0))  # Fill NaNs with 0 for similarity computation\n",
        "\n",
        "# Create a DataFrame for the similarity matrix\n",
        "cosine_sim_df = pd.DataFrame(cosine_sim, index=data.index, columns=data.index)\n",
        "\n",
        "# Step 3: Predict Ratings without Discount Factor\n",
        "def predict_rating_no_df(item, user):\n",
        "    # Get the top 20% most similar items for the target item\n",
        "    similar_items = cosine_sim_df[item].sort_values(ascending=False).head(int(len(cosine_sim_df) * 0.2))\n",
        "\n",
        "    # Compute the weighted average of ratings based on similarity\n",
        "    weighted_ratings = 0\n",
        "    similarity_sum = 0\n",
        "    for similar_item, similarity in similar_items.items():\n",
        "        rating = data.loc[similar_item, user]\n",
        "        if not pd.isna(rating):  # Only consider non-NaN ratings\n",
        "            weighted_ratings += similarity * rating\n",
        "            similarity_sum += similarity\n",
        "\n",
        "    if similarity_sum == 0:  # Prevent division by zero\n",
        "        return np.nan\n",
        "    return weighted_ratings / similarity_sum\n",
        "\n",
        "# Step 4: Apply Discount Factor and Predict Ratings\n",
        "def predict_rating_with_df(item, user, discount_factor_func=None):\n",
        "    # Get the top 20% most similar items for the target item\n",
        "    similar_items = cosine_sim_df[item].sort_values(ascending=False).head(int(len(cosine_sim_df) * 0.2))\n",
        "\n",
        "    # Compute the discounted similarity\n",
        "    weighted_ratings = 0\n",
        "    similarity_sum = 0\n",
        "    for similar_item, similarity in similar_items.items():\n",
        "        rating = data.loc[similar_item, user]\n",
        "        if not pd.isna(rating):  # Only consider non-NaN ratings\n",
        "            # Apply the discount factor if provided\n",
        "            discount_factor = discount_factor_func(similarity) if discount_factor_func else 1\n",
        "            weighted_ratings += similarity * discount_factor * rating\n",
        "            similarity_sum += similarity * discount_factor\n",
        "\n",
        "    if similarity_sum == 0:\n",
        "        return np.nan\n",
        "    return weighted_ratings / similarity_sum\n",
        "\n",
        "# Example discount factor function (based on cosine similarity)\n",
        "def discount_factor(similarity):\n",
        "    return 1 / (1 + np.exp(-similarity))  # Logistic function to reduce impact of less similar items\n",
        "\n",
        "# Step 5: Loop through Active Users and Predict Missing Ratings\n",
        "active_users = ['User1', 'User2', 'User3']\n",
        "target_items = data.index  # All items for which we need to predict ratings\n",
        "\n",
        "# For each active user, calculate predictions only for missing values\n",
        "for user in active_users:\n",
        "    print(f\"\\nPredictions for {user}:\")\n",
        "    for item in target_items:\n",
        "        if pd.isna(data.loc[item, user]):  # Only predict for missing values\n",
        "            # Prediction without discount factor\n",
        "            pred_no_df = predict_rating_no_df(item, user)\n",
        "            # Prediction with discount factor\n",
        "            pred_with_df = predict_rating_with_df(item, user, discount_factor)\n",
        "\n",
        "            print(f\"Item: {item}\")\n",
        "            print(f\"Predicted Rating without Discount Factor: {pred_no_df}\")\n",
        "            print(f\"Predicted Rating with Discount Factor: {pred_with_df}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez5Jg5WeuE5v",
        "outputId": "920114cd-d4dc-49a2-ce73-919d766f655e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions for User1:\n",
            "Item: Come the Morning\n",
            "Predicted Rating without Discount Factor: 3.70196579008071\n",
            "Predicted Rating with Discount Factor: 3.713687130395767\n",
            "Item: The Perfect Shadow\n",
            "Predicted Rating without Discount Factor: 2.4927189660999627\n",
            "Predicted Rating with Discount Factor: 2.5061930042394516\n",
            "Item: Maybe It's True What They Say About Us\n",
            "Predicted Rating without Discount Factor: 3.894107633686491\n",
            "Predicted Rating with Discount Factor: 3.9039141328748124\n",
            "Item: The Problem with People\n",
            "Predicted Rating without Discount Factor: 3.436144920613629\n",
            "Predicted Rating with Discount Factor: 3.401724784947061\n",
            "Item: Amityville: Where the Echo Lives\n",
            "Predicted Rating without Discount Factor: 3.645129493383592\n",
            "Predicted Rating with Discount Factor: 3.6262249947958076\n",
            "\n",
            "Predictions for User2:\n",
            "Item: I giganti del cielo\n",
            "Predicted Rating without Discount Factor: 2.9626159866965045\n",
            "Predicted Rating with Discount Factor: 2.979791595101329\n",
            "Item: L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\n",
            "Predicted Rating without Discount Factor: 2.3445110160859954\n",
            "Predicted Rating with Discount Factor: 2.3462063300157445\n",
            "Item: Power Alley\n",
            "Predicted Rating without Discount Factor: 2.743370536256612\n",
            "Predicted Rating with Discount Factor: 2.693960954175516\n",
            "\n",
            "Predictions for User3:\n",
            "Item: Nine Ball\n",
            "Predicted Rating without Discount Factor: 3.4937055923045732\n",
            "Predicted Rating with Discount Factor: 3.504232441244316\n",
            "Item: Moe\n",
            "Predicted Rating without Discount Factor: 3.6890008639558243\n",
            "Predicted Rating with Discount Factor: 3.6923043816805885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the top 20% closest users to each active user."
      ],
      "metadata": {
        "id": "hgHuY8p9KGtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Step 1: Mean-Centering - Adjust ratings by subtracting the mean for each user\n",
        "mean_ratings = user_item_matrix.mean(axis=1)  # Compute the mean rating for each user\n",
        "user_item_matrix_centered = user_item_matrix.sub(mean_ratings, axis=0)  # Subtract mean rating from each user\n",
        "\n",
        "# Step 2: Fill NaN with 0 for cosine similarity calculations\n",
        "user_item_matrix_centered_filled = user_item_matrix_centered.fillna(0)\n",
        "\n",
        "# Step 3: Compute Cosine Similarity between users\n",
        "similarity_matrix = pd.DataFrame(\n",
        "    cosine_similarity(user_item_matrix_centered_filled),\n",
        "    index=user_item_matrix_centered_filled.index,\n",
        "    columns=user_item_matrix_centered_filled.index\n",
        ")\n",
        "\n",
        "# Function to find top N% closest users based on cosine similarity\n",
        "def get_top_n_percent(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on cosine similarity.\n",
        "    \"\"\"\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)  # Sort users by similarity (highest first)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Step 4: Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nTop 20% Closest Users for {active_user}:\\n\")\n",
        "\n",
        "    # Get the similarity row for the active user\n",
        "    similarity_row = similarity_matrix.loc[active_user]\n",
        "\n",
        "    # Get the top 20% closest users\n",
        "    top_20_percent_users = get_top_n_percent(similarity_row, n=20)\n",
        "\n",
        "    # Display the top 20% closest users\n",
        "    print(top_20_percent_users)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqrWCjneKHO8",
        "outputId": "9de7833b-cdf2-433d-8a0e-33a0143b107d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20% Closest Users for User1:\n",
            "\n",
            "User1     1.000000\n",
            "User34    0.335968\n",
            "User16    0.279364\n",
            "User10    0.274481\n",
            "User9     0.242951\n",
            "User6     0.242320\n",
            "User5     0.155236\n",
            "User29    0.147421\n",
            "User23    0.140747\n",
            "User15    0.129336\n",
            "Name: User1, dtype: float64\n",
            "\n",
            "Top 20% Closest Users for User2:\n",
            "\n",
            "User2     1.000000\n",
            "User21    0.456713\n",
            "User28    0.444823\n",
            "User38    0.432251\n",
            "User43    0.389256\n",
            "User45    0.304393\n",
            "User25    0.247108\n",
            "User5     0.241456\n",
            "User19    0.216982\n",
            "User39    0.191035\n",
            "Name: User2, dtype: float64\n",
            "\n",
            "Top 20% Closest Users for User3:\n",
            "\n",
            "User3     1.000000\n",
            "User24    0.575759\n",
            "User12    0.275748\n",
            "User26    0.267658\n",
            "User47    0.254834\n",
            "User35    0.238089\n",
            "User17    0.222731\n",
            "User42    0.204389\n",
            "User22    0.194101\n",
            "User32    0.123002\n",
            "Name: User3, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the prediction for each active user to find whether the user will like or dislike the not yet seen or rated items."
      ],
      "metadata": {
        "id": "348xPHviKc91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(\"user_item_ratings.csv\", index_col=0)\n",
        "\n",
        "# Handle missing values (NaN) by filling with 0 or using another method\n",
        "data_filled = data.fillna(0)\n",
        "\n",
        "# Step 1: Cosine Similarity Calculation Between Users\n",
        "# Compute the cosine similarity matrix between users\n",
        "cosine_sim_users = cosine_similarity(data_filled.T)  # Transpose to compute similarity between users\n",
        "\n",
        "# Create a DataFrame for the similarity matrix (between users)\n",
        "cosine_sim_users_df = pd.DataFrame(cosine_sim_users, index=data.columns, columns=data.columns)\n",
        "\n",
        "# Step 2: Get Top 20% Closest Users for Each Active User\n",
        "def get_top_n_percent_users(similarity_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a similarity row for a user, return the top N% closest users based on Cosine Similarity.\n",
        "    \"\"\"\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Step 3: Predict Rating for Missing Items\n",
        "def predict_rating_for_item(item, user, top_users, similarity_matrix, ratings_matrix):\n",
        "    \"\"\"\n",
        "    Predict the rating for a missing item based on top N% closest users.\n",
        "    \"\"\"\n",
        "    weighted_ratings = 0\n",
        "    similarity_sum = 0\n",
        "    for similar_user, similarity in top_users.items():\n",
        "        rating = ratings_matrix.loc[item, similar_user]\n",
        "        if rating > 0:  # Only consider non-zero ratings\n",
        "            weighted_ratings += similarity * rating\n",
        "            similarity_sum += similarity\n",
        "\n",
        "    if similarity_sum == 0:\n",
        "        return 0  # If no similar ratings, return 0\n",
        "    return weighted_ratings / similarity_sum\n",
        "\n",
        "# Step 4: Loop through Active Users and Predict Missing Ratings\n",
        "active_users = ['User1', 'User2', 'User3']  # List of active users\n",
        "target_items = data.index  # All items for which we need to predict ratings\n",
        "\n",
        "# For each active user, calculate predictions only for missing values\n",
        "for user in active_users:\n",
        "    print(f\"\\nPredictions for {user}:\")\n",
        "\n",
        "    # Get the similarity row for the active user\n",
        "    similarity_row = cosine_sim_users_df[user]\n",
        "\n",
        "    # Get the top 20% closest users based on Cosine Similarity\n",
        "    top_users = get_top_n_percent_users(similarity_row, n=20)\n",
        "\n",
        "    for item in target_items:\n",
        "        if pd.isna(data.loc[item, user]):  # Only predict for missing values\n",
        "            # Predict the rating for the missing item\n",
        "            predicted_rating = predict_rating_for_item(item, user, top_users, cosine_sim_users_df, data)\n",
        "\n",
        "            # Assume a rating threshold of 3 to determine if the user will like or dislike the item\n",
        "            like_or_dislike = \"Like\" if predicted_rating >= 3 else \"Dislike\"\n",
        "\n",
        "            print(f\"Item: {item}\")\n",
        "            print(f\"Predicted Rating: {predicted_rating:.2f}\")\n",
        "            print(f\"Prediction: {like_or_dislike}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGzxW11vLszd",
        "outputId": "5ecef9bc-7284-4778-adf9-117ddffa6a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions for User1:\n",
            "Item: Come the Morning\n",
            "Predicted Rating: 2.89\n",
            "Prediction: Dislike\n",
            "Item: The Perfect Shadow\n",
            "Predicted Rating: 2.87\n",
            "Prediction: Dislike\n",
            "Item: Maybe It's True What They Say About Us\n",
            "Predicted Rating: 2.38\n",
            "Prediction: Dislike\n",
            "Item: The Problem with People\n",
            "Predicted Rating: 2.69\n",
            "Prediction: Dislike\n",
            "Item: Amityville: Where the Echo Lives\n",
            "Predicted Rating: 3.49\n",
            "Prediction: Like\n",
            "\n",
            "Predictions for User2:\n",
            "Item: I giganti del cielo\n",
            "Predicted Rating: 2.32\n",
            "Prediction: Dislike\n",
            "Item: L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\n",
            "Predicted Rating: 2.76\n",
            "Prediction: Dislike\n",
            "Item: Power Alley\n",
            "Predicted Rating: 3.22\n",
            "Prediction: Like\n",
            "\n",
            "Predictions for User3:\n",
            "Item: Nine Ball\n",
            "Predicted Rating: 2.67\n",
            "Prediction: Dislike\n",
            "Item: Moe\n",
            "Predicted Rating: 3.70\n",
            "Prediction: Like\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the discount factor (DF) then the discounted similarity (DS), for each of the active users considering the threshold in each case."
      ],
      "metadata": {
        "id": "F86hZEiWK9sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Step 1: Mean-Centering - Adjust ratings by subtracting the mean for each user\n",
        "mean_ratings = user_item_matrix.mean(axis=1)  # Compute the mean rating for each user\n",
        "user_item_matrix_centered = user_item_matrix.sub(mean_ratings, axis=0)  # Subtract mean rating from each user\n",
        "\n",
        "# Step 2: Fill NaN with 0 for cosine similarity calculations\n",
        "user_item_matrix_centered_filled = user_item_matrix_centered.fillna(0)\n",
        "\n",
        "# Step 3: Compute Cosine Similarity between users\n",
        "similarity_matrix = pd.DataFrame(\n",
        "    cosine_similarity(user_item_matrix_centered_filled),\n",
        "    index=user_item_matrix_centered_filled.index,\n",
        "    columns=user_item_matrix_centered_filled.index\n",
        ")\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Step 4: Function to compute Discount Factor (DF)\n",
        "def discount_factor(similarity, threshold=0.3):\n",
        "    \"\"\"\n",
        "    Compute the discount factor for a similarity score.\n",
        "    If similarity > threshold, use a logistic function, otherwise set DF to 0.\n",
        "    \"\"\"\n",
        "    if similarity > threshold:\n",
        "        return 1 / (1 + np.exp(-similarity))  # Logistic function\n",
        "    return 0  # Disregard similarities below the threshold\n",
        "\n",
        "# Step 5: Compute Discounted Similarity (DS)\n",
        "def compute_discounted_similarity(active_user, similarity_matrix, top_n_percent=20, threshold=0.3):\n",
        "    \"\"\"\n",
        "    Compute Discounted Similarity (DS) for the top N% closest users to an active user.\n",
        "    \"\"\"\n",
        "    # Get the similarity row for the active user\n",
        "    similarity_row = similarity_matrix.loc[active_user]\n",
        "\n",
        "    # Sort users by similarity and retain the top N% closest users\n",
        "    sorted_users = similarity_row.sort_values(ascending=False)\n",
        "    top_n_cutoff = int(len(sorted_users) * top_n_percent / 100)\n",
        "    top_similar_users = sorted_users.iloc[:top_n_cutoff]\n",
        "\n",
        "    # Compute Discounted Similarity (DS)\n",
        "    discounted_similarity = {}\n",
        "    for user, similarity in top_similar_users.items():\n",
        "        df = discount_factor(similarity, threshold)  # Compute discount factor\n",
        "        ds = similarity * df  # Compute discounted similarity\n",
        "        discounted_similarity[user] = (similarity, df, ds)\n",
        "\n",
        "    return discounted_similarity\n",
        "\n",
        "# Step 6: Apply to Active Users\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nDiscounted Similarity for {active_user}:\\n\")\n",
        "\n",
        "    # Compute discounted similarity for the active user\n",
        "    ds_results = compute_discounted_similarity(active_user, similarity_matrix, top_n_percent=20, threshold=0.3)\n",
        "\n",
        "    # Print results\n",
        "    for user, (similarity, df, ds) in ds_results.items():\n",
        "        print(f\"User: {user}, Similarity: {similarity:.4f}, DF: {df:.4f}, DS: {ds:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLlkz4NmMPBZ",
        "outputId": "71623a2a-4973-4213-e8e2-c61e12cf7982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Discounted Similarity for User1:\n",
            "\n",
            "User: User1, Similarity: 1.0000, DF: 0.7311, DS: 0.7311\n",
            "User: User34, Similarity: 0.3360, DF: 0.5832, DS: 0.1959\n",
            "User: User16, Similarity: 0.2794, DF: 0.0000, DS: 0.0000\n",
            "User: User10, Similarity: 0.2745, DF: 0.0000, DS: 0.0000\n",
            "User: User9, Similarity: 0.2430, DF: 0.0000, DS: 0.0000\n",
            "User: User6, Similarity: 0.2423, DF: 0.0000, DS: 0.0000\n",
            "User: User5, Similarity: 0.1552, DF: 0.0000, DS: 0.0000\n",
            "User: User29, Similarity: 0.1474, DF: 0.0000, DS: 0.0000\n",
            "User: User23, Similarity: 0.1407, DF: 0.0000, DS: 0.0000\n",
            "User: User15, Similarity: 0.1293, DF: 0.0000, DS: 0.0000\n",
            "\n",
            "Discounted Similarity for User2:\n",
            "\n",
            "User: User2, Similarity: 1.0000, DF: 0.7311, DS: 0.7311\n",
            "User: User21, Similarity: 0.4567, DF: 0.6122, DS: 0.2796\n",
            "User: User28, Similarity: 0.4448, DF: 0.6094, DS: 0.2711\n",
            "User: User38, Similarity: 0.4323, DF: 0.6064, DS: 0.2621\n",
            "User: User43, Similarity: 0.3893, DF: 0.5961, DS: 0.2320\n",
            "User: User45, Similarity: 0.3044, DF: 0.5755, DS: 0.1752\n",
            "User: User25, Similarity: 0.2471, DF: 0.0000, DS: 0.0000\n",
            "User: User5, Similarity: 0.2415, DF: 0.0000, DS: 0.0000\n",
            "User: User19, Similarity: 0.2170, DF: 0.0000, DS: 0.0000\n",
            "User: User39, Similarity: 0.1910, DF: 0.0000, DS: 0.0000\n",
            "\n",
            "Discounted Similarity for User3:\n",
            "\n",
            "User: User3, Similarity: 1.0000, DF: 0.7311, DS: 0.7311\n",
            "User: User24, Similarity: 0.5758, DF: 0.6401, DS: 0.3685\n",
            "User: User12, Similarity: 0.2757, DF: 0.0000, DS: 0.0000\n",
            "User: User26, Similarity: 0.2677, DF: 0.0000, DS: 0.0000\n",
            "User: User47, Similarity: 0.2548, DF: 0.0000, DS: 0.0000\n",
            "User: User35, Similarity: 0.2381, DF: 0.0000, DS: 0.0000\n",
            "User: User17, Similarity: 0.2227, DF: 0.0000, DS: 0.0000\n",
            "User: User42, Similarity: 0.2044, DF: 0.0000, DS: 0.0000\n",
            "User: User22, Similarity: 0.1941, DF: 0.0000, DS: 0.0000\n",
            "User: User32, Similarity: 0.1230, DF: 0.0000, DS: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the top 20% closest users using Discounted Similarity\n",
        "\n"
      ],
      "metadata": {
        "id": "5gh8NyLHK_KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Step 1: Mean-Centering - Adjust ratings by subtracting the mean for each user\n",
        "mean_ratings = user_item_matrix.mean(axis=1)  # Compute the mean rating for each user\n",
        "user_item_matrix_centered = user_item_matrix.sub(mean_ratings, axis=0)  # Subtract mean rating from each user\n",
        "\n",
        "# Step 2: Fill NaN with 0 for cosine similarity calculations\n",
        "user_item_matrix_centered_filled = user_item_matrix_centered.fillna(0)\n",
        "\n",
        "# Step 3: Compute Cosine Similarity between users\n",
        "similarity_matrix = pd.DataFrame(\n",
        "    cosine_similarity(user_item_matrix_centered_filled),\n",
        "    index=user_item_matrix_centered_filled.index,\n",
        "    columns=user_item_matrix_centered_filled.index\n",
        ")\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Step 4: Function to compute Discount Factor (DF)\n",
        "def discount_factor(similarity, threshold=0.3):\n",
        "    \"\"\"\n",
        "    Compute the discount factor for a similarity score.\n",
        "    If similarity > threshold, use a logistic function, otherwise set DF to 0.\n",
        "    \"\"\"\n",
        "    if similarity > threshold:\n",
        "        return 1 / (1 + np.exp(-similarity))  # Logistic function\n",
        "    return 0  # Disregard similarities below the threshold\n",
        "\n",
        "# Step 5: Function to compute Discounted Similarity (DS) and find top 20% closest users\n",
        "def top_20_percent_discounted_similarity(active_user, similarity_matrix, top_n_percent=20, threshold=0.3):\n",
        "    \"\"\"\n",
        "    Compute the top N% closest users to the active user based on discounted similarity (DS).\n",
        "    \"\"\"\n",
        "    # Get the similarity row for the active user\n",
        "    similarity_row = similarity_matrix.loc[active_user]\n",
        "\n",
        "    # Compute discounted similarity (DS) for each user\n",
        "    ds_values = {}\n",
        "    for user, similarity in similarity_row.items():\n",
        "        if user != active_user:  # Exclude self-similarity\n",
        "            df = discount_factor(similarity, threshold)  # Compute discount factor\n",
        "            ds = similarity * df  # Compute discounted similarity\n",
        "            ds_values[user] = ds\n",
        "\n",
        "    # Sort by discounted similarity (highest first)\n",
        "    sorted_ds = pd.Series(ds_values).sort_values(ascending=False)\n",
        "\n",
        "    # Retain top N% users\n",
        "    top_n_cutoff = int(len(sorted_ds) * top_n_percent / 100)\n",
        "    return sorted_ds.iloc[:top_n_cutoff]\n",
        "\n",
        "# Step 6: Apply to Active Users\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nTop 20% Closest Users based on Discounted Similarity for {active_user}:\\n\")\n",
        "\n",
        "    # Compute the top 20% closest users for the active user\n",
        "    top_20_users = top_20_percent_discounted_similarity(active_user, similarity_matrix, top_n_percent=20, threshold=0.3)\n",
        "\n",
        "    # Print results\n",
        "    print(top_20_users)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHl_Df1ROb6C",
        "outputId": "2355dfb6-0c65-4b15-bca1-9d8261a9ff5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20% Closest Users based on Discounted Similarity for User1:\n",
            "\n",
            "User34    0.19594\n",
            "User2    -0.00000\n",
            "User27   -0.00000\n",
            "User29    0.00000\n",
            "User30   -0.00000\n",
            "User31   -0.00000\n",
            "User32   -0.00000\n",
            "User33   -0.00000\n",
            "User35   -0.00000\n",
            "dtype: float64\n",
            "\n",
            "Top 20% Closest Users based on Discounted Similarity for User2:\n",
            "\n",
            "User21    0.279615\n",
            "User28    0.271079\n",
            "User38    0.262122\n",
            "User43    0.232037\n",
            "User45    0.175183\n",
            "User37    0.000000\n",
            "User29   -0.000000\n",
            "User30    0.000000\n",
            "User31   -0.000000\n",
            "dtype: float64\n",
            "\n",
            "Top 20% Closest Users based on Discounted Similarity for User3:\n",
            "\n",
            "User24    0.368538\n",
            "User1    -0.000000\n",
            "User38   -0.000000\n",
            "User29    0.000000\n",
            "User30    0.000000\n",
            "User31   -0.000000\n",
            "User32    0.000000\n",
            "User33    0.000000\n",
            "User34   -0.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the prediction for each active user to find whether the user will like or dislike the not yet seen or rated items."
      ],
      "metadata": {
        "id": "hs7EKAptLChj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_rating(active_user, item, top_users, user_item_matrix, similarity_matrix):\n",
        "    \"\"\"\n",
        "    Predict the rating for a given active user and item based on top 20% closest users.\n",
        "\n",
        "    Parameters:\n",
        "    - active_user: The target user for prediction\n",
        "    - item: The target item for prediction\n",
        "    - top_users: Top 20% closest users to the active user based on DS\n",
        "    - user_item_matrix: Original user-item ratings matrix\n",
        "    - similarity_matrix: User-user similarity matrix\n",
        "\n",
        "    Returns:\n",
        "    - Predicted rating for the active user on the target item\n",
        "    \"\"\"\n",
        "    numerator = 0\n",
        "    denominator = 0\n",
        "\n",
        "    for similar_user, discounted_similarity in top_users.items():\n",
        "        rating = user_item_matrix.loc[similar_user, item]\n",
        "        if not pd.isna(rating):  # Only consider users who rated the item\n",
        "            numerator += discounted_similarity * rating\n",
        "            denominator += discounted_similarity\n",
        "\n",
        "    if denominator == 0:  # Avoid division by zero\n",
        "        return np.nan\n",
        "\n",
        "    return numerator / denominator\n",
        "\n",
        "\n",
        "def like_or_dislike(predicted_rating, threshold=3.0):\n",
        "    \"\"\"\n",
        "    Determine whether a user will like or dislike an item based on the predicted rating.\n",
        "    \"\"\"\n",
        "    if pd.isna(predicted_rating):\n",
        "        return \"Unknown\"  # Cannot determine like/dislike for missing prediction\n",
        "    return \"Like\" if predicted_rating > threshold else \"Dislike\"\n",
        "\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# List of items (columns in the user-item matrix)\n",
        "items = user_item_matrix.columns\n",
        "\n",
        "# Step 1: Predict ratings for each active user and item\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nPredictions for {active_user}:\")\n",
        "\n",
        "    # Get the top 20% closest users for the active user\n",
        "    top_users = top_20_percent_discounted_similarity(active_user, similarity_matrix, top_n_percent=20, threshold=0.3)\n",
        "\n",
        "    for item in items:\n",
        "        if pd.isna(user_item_matrix.loc[active_user, item]):  # Only predict for unseen items\n",
        "            # Predict the rating\n",
        "            predicted_rating = predict_rating(active_user, item, top_users, user_item_matrix, similarity_matrix)\n",
        "\n",
        "            # Determine like or dislike\n",
        "            decision = like_or_dislike(predicted_rating)\n",
        "\n",
        "            print(f\"Item: {item} | Predicted Rating: {predicted_rating:.2f} | Decision: {decision}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2CVlsriOfPh",
        "outputId": "d3ece180-00ea-4ab0-b27d-d2c4c5d9abb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions for User1:\n",
            "Item: Come the Morning | Predicted Rating: 5.00 | Decision: Like\n",
            "Item: The Perfect Shadow | Predicted Rating: 1.00 | Decision: Dislike\n",
            "Item: Maybe It's True What They Say About Us | Predicted Rating: 1.00 | Decision: Dislike\n",
            "Item: The Problem with People | Predicted Rating: 4.00 | Decision: Like\n",
            "Item: Amityville: Where the Echo Lives | Predicted Rating: 4.00 | Decision: Like\n",
            "\n",
            "Predictions for User2:\n",
            "Item: I giganti del cielo | Predicted Rating: 2.17 | Decision: Dislike\n",
            "Item: L'homme au bÃ¢ton, une lÃ©gende crÃ©ole | Predicted Rating: 3.41 | Decision: Like\n",
            "Item: Power Alley | Predicted Rating: 3.00 | Decision: Dislike\n",
            "\n",
            "Predictions for User3:\n",
            "Item: Nine Ball | Predicted Rating: 5.00 | Decision: Like\n",
            "Item: Moe | Predicted Rating: nan | Decision: Unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case Study 2.3**"
      ],
      "metadata": {
        "id": "HuT1Z6pguvBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Item-based Collaborative Filtering algorithms using Pearson Correlation\n",
        "Coefficient (PCC)"
      ],
      "metadata": {
        "id": "LGQ3Wgc8uykr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset (assuming a matrix format with users as columns and items as rows)\n",
        "data = pd.read_csv(\"user_item_ratings.csv\", index_col=0)\n",
        "\n",
        "# Step 1: Calculate Pearson Correlation Coefficient (PCC) between items\n",
        "def pearson_correlation(item1, item2):\n",
        "    # Subtract mean ratings for each item\n",
        "    mean_item1 = data.loc[item1].mean()\n",
        "    mean_item2 = data.loc[item2].mean()\n",
        "\n",
        "    # Get the ratings for the two items\n",
        "    ratings_item1 = data.loc[item1] - mean_item1\n",
        "    ratings_item2 = data.loc[item2] - mean_item2\n",
        "\n",
        "    # Compute the Pearson correlation coefficient\n",
        "    numerator = np.sum(ratings_item1 * ratings_item2)\n",
        "    denominator = np.sqrt(np.sum(ratings_item1**2)) * np.sqrt(np.sum(ratings_item2**2))\n",
        "    return numerator / denominator if denominator != 0 else 0\n",
        "\n",
        "# Step 2: Compute the PCC matrix\n",
        "items = data.index\n",
        "pcc_matrix = pd.DataFrame(index=items, columns=items)\n",
        "\n",
        "for item1 in items:\n",
        "    for item2 in items:\n",
        "        pcc_matrix.loc[item1, item2] = pearson_correlation(item1, item2)\n",
        "\n",
        "# Step 3: Predict Rating without Discount Factor (PCC weighted average)\n",
        "def predict_rating_no_df(item, user):\n",
        "    # Get the top 20% most similar items based on PCC\n",
        "    similar_items = pcc_matrix[item].sort_values(ascending=False).head(int(len(pcc_matrix) * 0.2))\n",
        "\n",
        "    # Weighted average based on PCC\n",
        "    weighted_ratings = 0\n",
        "    similarity_sum = 0\n",
        "    for similar_item, similarity in similar_items.items():\n",
        "        rating = data.loc[similar_item, user]\n",
        "        if not pd.isna(rating):\n",
        "            weighted_ratings += similarity * rating\n",
        "            similarity_sum += similarity\n",
        "\n",
        "    if similarity_sum == 0:\n",
        "        return np.nan\n",
        "    return weighted_ratings / similarity_sum\n",
        "\n",
        "# Step 4: Discount Factor function (based on PCC)\n",
        "def discount_factor(pcc):\n",
        "    return 1 / (1 + np.exp(-pcc))  # Example of logistic decay\n",
        "\n",
        "# Step 5: Predict Rating with Discount Factor (Apply Discounted Similarity)\n",
        "def predict_rating_with_df(item, user, discount_factor_func=discount_factor):\n",
        "    # Get the top 20% most similar items based on PCC\n",
        "    similar_items = pcc_matrix[item].sort_values(ascending=False).head(int(len(pcc_matrix) * 0.2))\n",
        "\n",
        "    # Weighted average using discounted similarity\n",
        "    weighted_ratings = 0\n",
        "    similarity_sum = 0\n",
        "    for similar_item, similarity in similar_items.items():\n",
        "        rating = data.loc[similar_item, user]\n",
        "        if not pd.isna(rating):\n",
        "            # Apply discount factor to similarity\n",
        "            discounted_similarity = discount_factor_func(similarity)\n",
        "            weighted_ratings += discounted_similarity * rating\n",
        "            similarity_sum += discounted_similarity\n",
        "\n",
        "    if similarity_sum == 0:\n",
        "        return np.nan\n",
        "    return weighted_ratings / similarity_sum\n",
        "\n",
        "# Example: Predict ratings for active users with missing values\n",
        "active_users = ['User1', 'User2', 'User3']\n",
        "items = data.index\n",
        "\n",
        "for user in active_users:\n",
        "    print(f\"\\nPredictions for {user}:\")\n",
        "    for item in items:\n",
        "        if pd.isna(data.loc[item, user]):  # Only predict for missing values\n",
        "            # Prediction without discount factor\n",
        "            pred_no_df = predict_rating_no_df(item, user)\n",
        "            # Prediction with discount factor\n",
        "            pred_with_df = predict_rating_with_df(item, user)\n",
        "\n",
        "            print(f\"Item: {item}\")\n",
        "            print(f\"Predicted Rating without Discount Factor: {pred_no_df}\")\n",
        "            print(f\"Predicted Rating with Discount Factor: {pred_with_df}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fSS47ZCuory",
        "outputId": "aa0abc1e-53d7-4738-ed1a-7c62c060e18d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions for User1:\n",
            "Item: Come the Morning\n",
            "Predicted Rating without Discount Factor: 3.7019657900807093\n",
            "Predicted Rating with Discount Factor: 3.610016336074027\n",
            "Item: The Perfect Shadow\n",
            "Predicted Rating without Discount Factor: 2.4927189660999627\n",
            "Predicted Rating with Discount Factor: 2.407424740363386\n",
            "Item: Maybe It's True What They Say About Us\n",
            "Predicted Rating without Discount Factor: 3.894107633686491\n",
            "Predicted Rating with Discount Factor: 3.759805114124726\n",
            "Item: The Problem with People\n",
            "Predicted Rating without Discount Factor: 3.43614492061363\n",
            "Predicted Rating with Discount Factor: 3.718516592136105\n",
            "Item: Amityville: Where the Echo Lives\n",
            "Predicted Rating without Discount Factor: 3.6451294933835925\n",
            "Predicted Rating with Discount Factor: 3.785359991098569\n",
            "\n",
            "Predictions for User2:\n",
            "Item: I giganti del cielo\n",
            "Predicted Rating without Discount Factor: 2.962615986696505\n",
            "Predicted Rating with Discount Factor: 2.774797515434346\n",
            "Item: L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\n",
            "Predicted Rating without Discount Factor: 2.3445110160859954\n",
            "Predicted Rating with Discount Factor: 2.4895406728747194\n",
            "Item: Power Alley\n",
            "Predicted Rating without Discount Factor: 2.7433705362566125\n",
            "Predicted Rating with Discount Factor: 3.3434649094569364\n",
            "\n",
            "Predictions for User3:\n",
            "Item: Nine Ball\n",
            "Predicted Rating without Discount Factor: 3.493705592304572\n",
            "Predicted Rating with Discount Factor: 3.4103999056466696\n",
            "Item: Moe\n",
            "Predicted Rating without Discount Factor: 3.6890008639558247\n",
            "Predicted Rating with Discount Factor: 3.6066826575798574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine the top 20% closest users to each active user."
      ],
      "metadata": {
        "id": "TJCgUAxLKRE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the user-item ratings dataset\n",
        "ratings_file = \"user_item_ratings.csv\"\n",
        "data = pd.read_csv(ratings_file)\n",
        "\n",
        "# Transpose the dataset to make rows users and columns items\n",
        "user_item_matrix = data.set_index(data.columns[0]).T\n",
        "\n",
        "# Active users to analyze\n",
        "active_users = [\"User1\", \"User2\", \"User3\"]\n",
        "\n",
        "# Step 1: Compute the Pearson Correlation Coefficient (PCC) matrix\n",
        "def compute_pcc(user1, user2):\n",
        "    \"\"\"\n",
        "    Compute the Pearson Correlation Coefficient (PCC) between two users.\n",
        "    \"\"\"\n",
        "    # Extract ratings for both users\n",
        "    ratings_user1 = user_item_matrix.loc[user1]\n",
        "    ratings_user2 = user_item_matrix.loc[user2]\n",
        "\n",
        "    # Align ratings (only consider items rated by both users)\n",
        "    common_ratings = ratings_user1.dropna().index.intersection(ratings_user2.dropna().index)\n",
        "    if len(common_ratings) == 0:  # No common ratings\n",
        "        return 0\n",
        "\n",
        "    # Subtract mean ratings\n",
        "    ratings_user1 = ratings_user1[common_ratings] - ratings_user1[common_ratings].mean()\n",
        "    ratings_user2 = ratings_user2[common_ratings] - ratings_user2[common_ratings].mean()\n",
        "\n",
        "    # Compute PCC\n",
        "    numerator = np.sum(ratings_user1 * ratings_user2)\n",
        "    denominator = np.sqrt(np.sum(ratings_user1**2)) * np.sqrt(np.sum(ratings_user2**2))\n",
        "    return numerator / denominator if denominator != 0 else 0\n",
        "\n",
        "# Create the PCC similarity matrix\n",
        "users = user_item_matrix.index\n",
        "pcc_matrix = pd.DataFrame(index=users, columns=users)\n",
        "\n",
        "for user1 in users:\n",
        "    for user2 in users:\n",
        "        if user1 != user2:\n",
        "            pcc_matrix.loc[user1, user2] = compute_pcc(user1, user2)\n",
        "        else:\n",
        "            pcc_matrix.loc[user1, user2] = 0  # Self-similarity is 0\n",
        "\n",
        "# Step 2: Get top 20% closest users for each active user\n",
        "def get_top_n_percent_pcc(pcc_row, n=20):\n",
        "    \"\"\"\n",
        "    Given a PCC similarity row for a user, return the top N% closest users based on PCC.\n",
        "    \"\"\"\n",
        "    sorted_users = pcc_row.sort_values(ascending=False)  # Sort users by PCC (highest first)\n",
        "    top_n_cutoff = int(len(sorted_users) * n / 100)  # Calculate the number of top N% users\n",
        "    return sorted_users.iloc[:top_n_cutoff]  # Return the top N% closest users\n",
        "\n",
        "# Step 3: Perform analysis for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nTop 20% Closest Users for {active_user} (based on PCC):\\n\")\n",
        "\n",
        "    # Get the PCC row for the active user\n",
        "    pcc_row = pcc_matrix.loc[active_user].astype(float)\n",
        "\n",
        "    # Get the top 20% closest users\n",
        "    top_20_percent_users = get_top_n_percent_pcc(pcc_row, n=20)\n",
        "\n",
        "    print(top_20_percent_users)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxjeDWZCQPhJ",
        "outputId": "0c88b9dc-aa7e-46dc-f37b-dc755d186121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20% Closest Users for User1 (based on PCC):\n",
            "\n",
            "User34    0.398041\n",
            "User10    0.351821\n",
            "User16    0.317940\n",
            "User9     0.281516\n",
            "User6     0.251077\n",
            "User5     0.182774\n",
            "User23    0.173773\n",
            "User29    0.163077\n",
            "User15    0.148594\n",
            "User44    0.146160\n",
            "Name: User1, dtype: float64\n",
            "\n",
            "Top 20% Closest Users for User2 (based on PCC):\n",
            "\n",
            "User21    0.522427\n",
            "User28    0.512616\n",
            "User38    0.483870\n",
            "User43    0.415225\n",
            "User45    0.329009\n",
            "User5     0.289938\n",
            "User25    0.264203\n",
            "User19    0.232580\n",
            "User39    0.208653\n",
            "User42    0.189348\n",
            "Name: User2, dtype: float64\n",
            "\n",
            "Top 20% Closest Users for User3 (based on PCC):\n",
            "\n",
            "User24    0.641427\n",
            "User26    0.305876\n",
            "User35    0.284052\n",
            "User12    0.278241\n",
            "User47    0.271917\n",
            "User42    0.241293\n",
            "User17    0.238366\n",
            "User22    0.202948\n",
            "User32    0.132131\n",
            "User10    0.129600\n",
            "Name: User3, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the prediction for each active user to find whether the user will like or dislike the not yet seen or rated items."
      ],
      "metadata": {
        "id": "KmmWOfL-KbQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_ratings_for_active_user(active_user, unrated_items, top_users, user_item_matrix):\n",
        "    \"\"\"\n",
        "    Predict the ratings for the active user's unrated items based on their top similar users.\n",
        "    \"\"\"\n",
        "    predictions = {}\n",
        "    for item in unrated_items:\n",
        "        # Collect ratings from top similar users for the given item\n",
        "        weighted_sum = 0\n",
        "        similarity_sum = 0\n",
        "\n",
        "        for similar_user, similarity in top_users.items():\n",
        "            rating = user_item_matrix.loc[similar_user, item]\n",
        "            if not pd.isna(rating):  # Use only valid ratings\n",
        "                weighted_sum += similarity * rating\n",
        "                similarity_sum += abs(similarity)  # Use absolute similarity to avoid negative weights\n",
        "\n",
        "        # Compute the predicted rating\n",
        "        if similarity_sum > 0:\n",
        "            predicted_rating = weighted_sum / similarity_sum\n",
        "        else:\n",
        "            predicted_rating = np.nan  # Not enough information to predict\n",
        "\n",
        "        predictions[item] = predicted_rating\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def classify_like_dislike(predictions, threshold=3.5):\n",
        "    \"\"\"\n",
        "    Classify items as 'like' or 'dislike' based on the prediction threshold.\n",
        "    \"\"\"\n",
        "    classification = {}\n",
        "    for item, rating in predictions.items():\n",
        "        if pd.isna(rating):\n",
        "            classification[item] = \"unknown\"  # Unable to classify\n",
        "        elif rating >= threshold:\n",
        "            classification[item] = \"like\"\n",
        "        else:\n",
        "            classification[item] = \"dislike\"\n",
        "\n",
        "    return classification\n",
        "\n",
        "\n",
        "# Step 1: Identify unrated items for each active user\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nPredictions for {active_user}:\")\n",
        "\n",
        "    # Get unrated items\n",
        "    unrated_items = user_item_matrix.columns[user_item_matrix.loc[active_user].isna()]\n",
        "\n",
        "    # Get top 20% closest users for the active user\n",
        "    pcc_row = pcc_matrix.loc[active_user].astype(float)\n",
        "    top_users = get_top_n_percent_pcc(pcc_row, n=20)  # Retrieve the top 20% closest users\n",
        "\n",
        "    # Step 2: Predict ratings for unrated items\n",
        "    predictions = predict_ratings_for_active_user(active_user, unrated_items, top_users, user_item_matrix)\n",
        "\n",
        "    # Step 3: Classify items as 'like' or 'dislike'\n",
        "    classification = classify_like_dislike(predictions, threshold=3.5)\n",
        "\n",
        "    # Output the results\n",
        "    for item, prediction in predictions.items():\n",
        "        print(f\"Item: {item}\")\n",
        "        print(f\"Predicted Rating: {prediction}\")\n",
        "        print(f\"Classification: {classification[item]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-oUM_jBQUII",
        "outputId": "8cbecc15-c6ea-4dac-c2c5-98dad37be2a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions for User1:\n",
            "Item: Come the Morning\n",
            "Predicted Rating: 3.6442197956076554\n",
            "Classification: like\n",
            "Item: The Perfect Shadow\n",
            "Predicted Rating: 1.9939397954300844\n",
            "Classification: dislike\n",
            "Item: Maybe It's True What They Say About Us\n",
            "Predicted Rating: 3.3990608236071593\n",
            "Classification: dislike\n",
            "Item: The Problem with People\n",
            "Predicted Rating: 3.622392408397928\n",
            "Classification: like\n",
            "Item: Amityville: Where the Echo Lives\n",
            "Predicted Rating: 3.851301949355619\n",
            "Classification: like\n",
            "\n",
            "Predictions for User2:\n",
            "Item: I giganti del cielo\n",
            "Predicted Rating: 2.57754015683355\n",
            "Classification: dislike\n",
            "Item: L'homme au bÃ¢ton, une lÃ©gende crÃ©ole\n",
            "Predicted Rating: 3.2057646199814567\n",
            "Classification: dislike\n",
            "Item: Power Alley\n",
            "Predicted Rating: 3.1884951697195025\n",
            "Classification: dislike\n",
            "\n",
            "Predictions for User3:\n",
            "Item: Nine Ball\n",
            "Predicted Rating: 4.243249582164848\n",
            "Classification: like\n",
            "Item: Moe\n",
            "Predicted Rating: 3.4797794732144536\n",
            "Classification: dislike\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the discount factor (DF) then the discounted similarity (DS), for each of the active users considering the threshold in each case."
      ],
      "metadata": {
        "id": "cKsuKTllKzDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discount_factor(pcc):\n",
        "    \"\"\"\n",
        "    Compute the Discount Factor (DF) based on the logistic decay function.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-pcc))\n",
        "\n",
        "\n",
        "def compute_discounted_similarity(pcc_matrix):\n",
        "    \"\"\"\n",
        "    Compute the Discounted Similarity (DS) matrix from the PCC matrix.\n",
        "    \"\"\"\n",
        "    ds_matrix = pcc_matrix.copy()\n",
        "    for user1 in pcc_matrix.index:\n",
        "        for user2 in pcc_matrix.columns:\n",
        "            original_pcc = pcc_matrix.loc[user1, user2]\n",
        "            df = discount_factor(original_pcc)\n",
        "            ds_matrix.loc[user1, user2] = original_pcc * df\n",
        "    return ds_matrix\n",
        "\n",
        "\n",
        "def get_top_n_discounted_similarity(active_user, ds_matrix, threshold=0.0, top_n_percent=20):\n",
        "    \"\"\"\n",
        "    For the active user, return the top N% closest users based on Discounted Similarity (DS).\n",
        "    Only include users where DS > threshold.\n",
        "    \"\"\"\n",
        "    # Filter DS values above the threshold\n",
        "    ds_values = ds_matrix.loc[active_user]\n",
        "    ds_filtered = ds_values[ds_values > threshold]\n",
        "\n",
        "    # Get the top N% closest users\n",
        "    sorted_ds = ds_filtered.sort_values(ascending=False)\n",
        "    top_n_cutoff = int(len(sorted_ds) * top_n_percent / 100)\n",
        "    return sorted_ds.iloc[:top_n_cutoff]\n",
        "\n",
        "\n",
        "# Step 1: Compute the DS matrix\n",
        "ds_matrix = compute_discounted_similarity(pcc_matrix)\n",
        "\n",
        "# Step 2: Find the top users for each active user based on DS\n",
        "threshold = 0.1  # Set a similarity threshold\n",
        "for active_user in active_users:\n",
        "    print(f\"\\nTop 20% closest users for {active_user} based on DS:\")\n",
        "    top_users_ds = get_top_n_discounted_similarity(active_user, ds_matrix, threshold=threshold, top_n_percent=20)\n",
        "    print(top_users_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdfNljyvQvFE",
        "outputId": "a0bc90d2-4fb8-4046-a66d-71ff87a97fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 20% closest users for User1 based on DS:\n",
            "User34    0.238115\n",
            "Name: User1, dtype: object\n",
            "\n",
            "Top 20% closest users for User2 based on DS:\n",
            "User21    0.327935\n",
            "User28      0.3206\n",
            "Name: User2, dtype: object\n",
            "\n",
            "Top 20% closest users for User3 based on DS:\n",
            "User24    0.420184\n",
            "Name: User3, dtype: object\n"
          ]
        }
      ]
    }
  ]
}